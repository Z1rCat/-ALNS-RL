# RL 诊断报告（阶段1-3期间）

## 1. 报告范围
- 目标：解释“收敛快/慢差异大、训练耗时长、阶段不切换”的原因，并给出可用 RL 算法与适应性排名。
- 数据来源：
  - `A:/MYpython/34959_RL/codes/logs/run_20260115_125200_R30_S5_1/rl_trace.csv`
  - `A:/MYpython/34959_RL/codes/logs/run_20260115_125200_R30_S5_1/rl_training.csv`

## 2. 关键现象（症状）
- 长时间停留在训练阶段，无法跳到 implement。
- 训练耗时显著变长，表现为“长时间无新输出”。
- 同一场景下，有时几张表就收敛，有时长期不收敛。

## 3. 数据证据（从日志提取）
- `table_number` 范围：仅到 **0–71**（尚未进入 S5 的 Phase B）。
- 最近一次评估：`avg_reward = 0.0`，`std_reward = 0.0`。
- 训练累计时间：约 **21190 秒**（`training_time`）。
- 真实 reward 统计（排除 `-10000000` 占位值）：
  - reward=1 与 reward=0 基本各占一半，平均约 **0.50**。

## 4. 诊断结论（核心原因）

### 4.1 评估样本过小 → 高方差
- 当前 `iteration_numbers_unit = 1`，评估样本极少。
- 评估样本“碰巧容易”时 `avg_reward` 会突然变高 → 快速收敛。
- 评估样本“偏难/不足”时 `avg_reward` 很低 → 长期不收敛。

### 4.2 非平稳环境 + 奖励噪声
- 分布切换 + ALNS 随机性导致环境强非平稳。
- 日志中存在 `wrong label` 提示，说明奖励与标签存在噪声。
- 非平稳 + 噪声叠加，会显著降低稳定收敛概率。

### 4.3 收敛门槛高，更新频率低
- 收敛条件：`avg_reward >= 0.9` 且 `success_times >= 3`。
- 但 `total_timesteps2 = 1`，每轮训练更新极少。
- 结果是“高门槛 + 少更新” → 长期卡在训练阶段。

### 4.4 个别表耗时过长
- 某些表对应的 ALNS 迭代复杂度高，单表耗时明显上升。
- 造成“看起来卡死”的错觉，但实际仍在计算该表。

## 5. 为什么“有时极速收敛，有时永不收敛”
这不是单纯算法问题，而是 **评估样本过小 + 强非平稳 + 奖励噪声 + 高门槛** 共同造成的高方差现象：
- 快速收敛：评估样本偶然偏易。
- 长期不收敛：评估样本偏难或数量不足。

## 6. 可用 RL 算法与适应性排名（当前环境）
> 当前 action space 为离散 (2)，连续动作算法不适配。

### 6.1 可用算法（与代码兼容）
- PPO
- DQN（当前使用）
- A2C

### 6.2 适应性排名（建议）
1. **PPO（可能效果最好）**
   - 优点：训练更稳定，对非平稳环境更鲁棒，抗奖励噪声。
   - 缺点：样本效率较低，但在你的场景可接受。

2. **DQN（中等）**
   - 优点：离散动作匹配，简单场景可快速收敛。
   - 缺点：对非平稳和噪声敏感，表现两极化。

3. **A2C（可能最差）**
   - 优点：结构简单。
   - 缺点：对高方差奖励敏感，稳定性往往弱于 PPO。

### 6.3 不适配算法
- DDPG / TD3 / SAC：连续动作算法，与 `Discrete(2)` 不兼容。
- HER：在 SB3 中是 replay buffer 类，不是独立算法。

## 7. 结论（简要）
- 当前“收敛快慢差异巨大”是机制导致的高方差，不是单一算法问题。
- 稳定性改进优先级：评估样本量与机制 > 收敛门槛 > 算法替换。
- 在现有环境下，PPO 通常更稳，DQN 易两极化，A2C 常表现最弱。
