# 实现报告：ALNS‑RL 系统接口与可扩展点（用于研发新RL算法）

> 目的：用“代码可定位”的方式说明当前 ALNS‑RL 系统的环境接口、训练/测试循环、跳级/漂移/日志钩子，以及**要插入新RL算法时应改哪里**。  
> 备注：下文只贴关键片段与路径，不贴全代码。

---

## (A) 环境与训练循环

### A1. 环境类路径与接口（Env API）

**环境类：**`coordinationEnv`  
**文件：**`codes/dynamic_RL34959.py`

环境是一个 SB3 `Env`，动作空间为 2 个离散动作（等待/重规划），观测空间为 2 或 3 维 Box：

```py
# codes/dynamic_RL34959.py
class coordinationEnv(Env):
    def __init__(self):
        self.action_space = Discrete(2)
        if add_event_types == 1:
            self.observation_space = Box(low=np.array([0, 0, 0]), high=np.array([200, 6, 6]))
        else:
            self.observation_space = Box(low=np.array([0, 0]), high=np.array([200, 6]))
```

### A2. `reset()` 返回的 obs 结构（shape / dtype / 是否含 phase/gt）

**文件：**`codes/dynamic_RL34959.py`

训练/实施阶段都通过共享表 `Intermodal_ALNS34959.state_reward_pairs` 获取“下一次需要决策的不确定事件”，`reset()` 会阻塞等待，直到找到 `action == -10000000` 的行，然后调用 `get_state()` 生成观测。

观测默认是 `np.array(dtype=float)`：
- shape：`(2,)`（默认）或 `(3,)`（`add_event_types==1`）
- dtype：`float`
- 字段：**`[delay_tolerance, severity_level]`**（可选再加 `event_type`）
- **不包含** `phase/gt_mean/gt_std`（这些目前仅用于日志，不进入 obs）

```py
# codes/dynamic_RL34959.py
def get_state(chosen_pair, ...):
    state_list[0] = chosen_pair[4]          # delay_tolerance
    ...
    duration_length = duration[1] - duration[0]
    severity_level = 1..6                   # 由 duration_length 分桶得到
    if add_event_types == 1:
        state_list = [state_list[0], severity_level, event_type]
    else:
        state_list = [state_list[0], severity_level]
    return np.array(state_list, dtype=float)
```

> `phase_label/gt_mean` 从每个 Excel 的 `__meta__` sheet 读取后，写入全局变量 `current_gt_mean/current_phase_label`，用于 `rl_trace.csv` 记录，但**不进入 obs**（见 C2/D）。

### A3. `step(action)` 的返回与 reward/done/info

**文件：**`codes/dynamic_RL34959.py`

在 `add_ALNS==1` 的真实仿真模式下：`step()` 会把动作写入共享表（发送给 ALNS），然后**轮询等待** ALNS 回填 reward（`reward != -10000000`）：

```py
# codes/dynamic_RL34959.py
def step(self, action):
    ...
    send_action(action)  # 写入 state_reward_pairs['action']
    while True:
        for pair_index in Intermodal_ALNS34959.state_reward_pairs.index:
            check = ... and state_reward_pairs['reward'][pair_index] != -10000000
            if check:
                reward = state_reward_pairs['reward'][pair_index]
                all_rewards_list.append(reward)
                recent_rewards.append(reward)
                step_id = next_step()
                log_training_row("train" or "implement", step_idx=step_id, reward=reward)
                log_trace_from_row(row_dict, "receive_reward", action=..., reward=reward)
                ...
                break
        if break_flag: break
    ...
    done = (self.horizon_length == episode_length)
    info = {}
    return self.state, reward, done, info
```

**reward 的定义（关键！）**由 ALNS 侧计算并写回 `state_reward_pairs['reward']`。目前 reward 主要是 **0/1 二元标签**（更像一个“是否应重规划”的分类回报）：

```py
# codes/Intermodal_ALNS34959.py
def get_reward(..., action, ... , bool_or_route, ...):
    if (action == 1 and isinstance(bool_or_route, bool)) or (action == 0 and not isinstance(bool_or_route, bool)):
        reward = 1
    else:
        reward = 0
    return reward
```

含义直观理解：
- `action=0`（等待/保持）在“可行/无需重规划”时给 `reward=1`
- `action=1`（重规划）在“不可行/需要重规划”时给 `reward=1`

**done 条件：**`self.horizon_length == episode_length`（默认 `episode_length=1`）  
**info：**当前固定 `{}`（没有额外诊断信息）

### A4. 一次 episode 的定义：500 个 table_number 还是每个 table_number 一步？

**结论：在当前实现里，“一个 table_number 对应一次决策步”，且默认一个 episode 只有 1 步。**

证据：
```py
# codes/dynamic_RL34959.py
episode_length = 1
...
if self.horizon_length == episode_length:
    done = True
```

这会导致 SB3 训练日志里经常出现 `ep_len_mean=1`（每步都终止并 reset）。

> 影响：如果你想做“自检测漂移/带记忆的策略（RNN/Transformer）”，当前 `episode_length=1` 会让**隐藏状态每步都被 done 重置**，模型很难利用跨步历史（详见 D 的建议）。

### A5. 测试阶段怎么切换 `implement=1`？测试为什么是“倒序读表”？

切换由 **ALNS 线程**触发（不是 SB3 自己）。核心通信变量：
- `dynamic_RL34959.curriculum_converged`：RL 侧判定收敛
- `dynamic_RL34959.implement`：进入实施/测试阶段的全局开关
- `Dynamic_ALNS_RL34959.table_number`：当前读取的环境文件编号

**倒序读表：**测试阶段从 `table_number=499` 递减到边界 `350`（共约 150 个文件）。

```py
# codes/Dynamic_ALNS_RL34959.py
if getattr(dynamic_RL34959, "implement", 0) == 1:
    if implement_start_synced == 0:
        if table_number < 499:
            table_number = 499
        implement_start_synced = 1
    Intermodal_ALNS_function(request_number_in_R)
    table_number -= 1
    if table_number < 350:
        print(">>> TEST COMPLETE: Reached boundary (350)...")
        ...
        return
```

训练阶段则主要是 `table_number` 递增，并在“跳级/curriculum”逻辑里跳转到不同阶段或进入测试（见 C1）。

---

## (B) SB3 使用方式（PPO/DQN/A2C）

### B1. 目前 SB3 是怎么创建与调用的？

**文件：**`codes/dynamic_RL34959.py`（`main()` 内）

```py
# codes/dynamic_RL34959.py
if algorithm == 'DQN':
    model = DQN('MlpPolicy', env, verbose=1, learning_starts=10, device='cpu', seed=seed_val)
elif algorithm == 'PPO':
    model = PPO('MlpPolicy', env, n_steps=10, verbose=1, device='cpu', seed=seed_val)
elif algorithm == 'A2C':
    model = A2C('MlpPolicy', env, n_steps=10, verbose=1, device='cpu', seed=seed_val)
```

**关键现状：**
- `device='cpu'`：当前训练默认在 CPU 上（GPU 基本没参与）。
- `learning_rate/gamma/ent_coef/clip_range/...` 未显式设置：大多沿用 SB3 默认值（除 `n_steps/learning_starts`）。

### B2. 训练步数 `timesteps` 的单位是什么？

当前训练循环以“极小步长”滚动：

```py
# codes/dynamic_RL34959.py
iteration_numbers_unit = 1
iteration_multiply = 1
total_timesteps2 = iteration_numbers_unit * iteration_multiply  # => 1

for _ in range(1000000000):
    if implement == 1:
        break
    model.learn(total_timesteps=total_timesteps2)  # 每次 learn 只跑 1 个 env step
    ...
```

因此：
- `timesteps` ≈ **决策次数**（每次不确定事件一次决策）
- 因 `episode_length=1`，训练表现更接近“上下文 bandit/分类”而非长序列控制

### B3. 是否使用 VecEnv / 多进程？

**SB3 环境层面：**未使用 `DummyVecEnv/SubprocVecEnv`，是单 env 实例：`env = coordinationEnv()`。  
**并行主要来源：**
- **一个 run 内：**`Dynamic_master34959.py` 用 `ThreadPoolExecutor` 同时跑 ALNS 线程（approach=0）与 RL 线程（approach=1），通过共享全局变量通信。
- **多个 run：**`codes/run_experiments_local.py` / `codes/run_experiments_server.py`（如有）在 run 粒度并发（多个进程各自独立 run_dir）。

---

## (C) 现有“跳级/漂移/日志”的钩子

### C1. curriculum jumping（智能跳级）在哪里实现？

这是一个**双端协作机制**：

1) **RL 侧：只负责判定“收敛/掌握”**（写 `curriculum_converged`）  
文件：`codes/dynamic_RL34959.py`

```py
# codes/dynamic_RL34959.py
rolling_avg = sum(recent_rewards) / len(recent_rewards) if recent_rewards else -1000
threshold = CURRICULUM_REWARD_THRESHOLD  # 默认 0.7，S0_Debug 特例 0.3
if rolling_avg >= threshold:
    sucess_times += 1
else:
    sucess_times = 0
curriculum_converged = 1 if sucess_times >= CURRICULUM_SUCCESS_REQUIRED else 0
```

2) **ALNS 侧：读取 `curriculum_converged` 决定跳转 table_number / 切换 implement**  
文件：`codes/Dynamic_ALNS_RL34959.py`

```py
# codes/Dynamic_ALNS_RL34959.py
converged = getattr(dynamic_RL34959, "curriculum_converged", 0) == 1
if converged:
    if scenario_pattern == "adaptation" or scenario_name.startswith("S5"):
        if table_number < 100:
            next_table_number = 100          # A -> B
        else:
            dynamic_RL34959.implement = 1
            next_table_number = 499          # -> Test
```

`scenario_pattern` 来源于根目录的 `distribution_config.json`（由 `Dynamic_ALNS_RL34959.py` 读取）。

### C2. RL 能否读到 `phase_label / gt_mean / gt_std`？

**当前：**
- `gt_mean/phase_label`：来自 Excel 的 `__meta__` sheet，在 `get_state()` 里读到并写入全局变量：
  - `dynamic_RL34959.current_gt_mean`
  - `dynamic_RL34959.current_phase_label`
- **但它们不进入 obs，仅进入日志 `rl_trace.csv`**
- `gt_std`：目前**没有写入 `__meta__`**，绘图脚本通常通过 `distribution_config.json` 的 `variance/std` 推导（见 D 的建议）。

```py
# codes/dynamic_RL34959.py
meta_df = pd.read_excel(Data, "__meta__")
current_gt_mean = meta_map.get("gt_mean", "")
current_phase_label = meta_map.get("phase_label", "")
```

### C3. 记录的日志结构（rl_trace / rl_training / baseline）

#### `rl_trace.csv`（轨迹日志）

**字段定义：**`codes/dynamic_RL34959.py` 的 `TRACE_FIELDS`：

```py
# codes/dynamic_RL34959.py
TRACE_FIELDS = [
  "ts","phase","stage","uncertainty_index","request","vehicle",
  "table_number","dynamic_t_begin","duration_type","gt_mean","phase_label",
  "delay_tolerance","severity","passed_terminals","current_time",
  "action","reward","action_meaning","feasible","source"
]
```

说明（抓重点）：
- `ts`：写日志的 wall-clock 时间戳（可用来估算每步耗时，见 E）
- `phase`：`train` / `implement`
- `stage`：如 `send_action / begin_removal / finish_removal / receive_reward / finish_insertion ...`
- `table_number`：当前环境文件编号
- `gt_mean / phase_label`：来自 Excel `__meta__`（见 C2）
- `source`：`RL / ALNS / BASELINE`（用于混合对比）

#### `rl_training.csv`（训练/评估摘要）

**字段：**`codes/dynamic_RL34959.py` 的 `TRAIN_FIELDS`：

```py
TRAIN_FIELDS = [
  "ts","phase","step_idx","reward","avg_reward","std_reward",
  "rolling_avg","recent_count","training_time","implementation_time"
]
```

#### `baseline_*.csv`（基准回放）

**生成脚本：**`codes/run_benchmark_replay.py`  
**字段：**复用 `dynamic_RL34959.TRACE_FIELDS`（与 `rl_trace.csv` 同构），用于直接叠加绘图/对比。

### C4. run_dir 物理隔离（每次运行独立目录）

**核心实现：**`codes/rl_logging.py` + `codes/Dynamic_master34959.py`

```py
# codes/Dynamic_master34959.py
rl_logging.set_run_dir(run_id)
os.environ["DYNAMIC_DATA_ROOT"] = str(rl_logging.get_run_data_dir())   # Excel 环境文件写入 run_dir/data
os.environ["ALNS_OUTPUT_ROOT"] = str(rl_logging.get_run_dir())         # ALNS 输出重定向到 run_dir
rl_logging.write_meta({...})                                           # run_dir/meta.json
```

因此每个 run 的典型结构：
```
codes/logs/run_YYYYMMDD_.../
  data/                      # 500 个 Excel（含 __meta__）
  meta.json
  rl_trace.csv
  rl_training.csv
  baseline_wait.csv / baseline_reroute.csv（如有）
  paper_figures/（绘图输出）
```

---

## (D) 插入新算法需要的“可观测漂移指标”

> 目标：为了研发“对不确定性/漂移适应更强”的新RL算法，你需要让策略可观测到**某种漂移信号**（或让算法具备自检测能力）。

### D1. 目前“漂移信号”怎么得到？

当前漂移信息本质上是**已知的环境生成机制**：
- `distribution_config.json` 定义了每个场景的 `pattern/means/variance/std`
- `codes/generate_mixed_parallel.py` 基于该配置为每个文件生成 `gt_mean/phase_label`，并写入 Excel 的 `__meta__`
- 运行时 `get_state()` 读取 `__meta__`，把 `gt_mean/phase_label` 写入全局变量用于记录

也就是说：系统已经具备“oracle 级别”的漂移标签，只是**还没把它给 RL 当作观测输入**。

### D2. 你希望漂移触发发生在“已知切换点”还是“模型自检测”？

两条路线（建议你先明确论文叙事与公平性）：

1) **已知（oracle）漂移**：直接把 `phase_label/gt_mean/gt_std`（或它们的变化量）喂给策略。  
   - 优点：实现简单，机制清晰；更像“条件策略/场景感知策略”。
   - 风险：与不使用该信息的 PPO/DQN 等对比时，可能被质疑“信息不公平”。

2) **未知（self-detected）漂移**：策略只看原始 obs（delay_tolerance/severity/...），通过历史统计/记忆网络自己发现分布变化。  
   - 优点：更符合“自适应/泛化”的 RL 研究主线。
   - 难点：需要跨步记忆，但当前 `episode_length=1` 会天然抹掉记忆（见 A4）。

### D3. 最小侵入式的插入点（从易到难）

#### 方案 1（最小改动）：把 `gt_mean` 加入 obs（oracle mean）

改动点：
1) `codes/dynamic_RL34959.py`：`get_state()` 返回时追加 `current_gt_mean`
2) 同文件：`coordinationEnv.__init__()` 的 `observation_space` 上界/维度同步

示意（伪代码）：
```py
# codes/dynamic_RL34959.py
state_list = [delay_tolerance, severity_level, float(current_gt_mean)]
return np.array(state_list, dtype=float)
```

#### 方案 2：加入 `gt_std`（应对“均值不变但方差变”的 V1 类场景）

实现路径二选一：
- **A：生成器写入**（推荐）  
  `codes/generate_mixed_parallel.py` 写 `__meta__` 时增加 `gt_std`（或 `gt_var`），运行时直接读取。
- **B：运行时推导**  
  运行时根据 `SCENARIO_NAME + phase_label` 去 `distribution_config.json` 查 `variance/std` 再计算。

> 你已经在绘图脚本里做过 `gt_std = sqrt(variance)` 的推导逻辑（`codes/plot_paper_figure.py`），同样逻辑可复用到环境侧。

#### 方案 3：自检测漂移（不依赖 meta 标签）

至少需要解决两个结构性问题：
1) **跨步信息**：当前 `episode_length=1`，done 每步触发，RNN 隐状态会被重置。  
   - 最小改动是把 `episode_length` 调大，并确保环境在一个 episode 内持续给出下一次决策状态（需要重新定义 episode 边界）。
2) **漂移特征**：可在 obs 中加入“滚动统计量”，例如：
   - `rolling_mean(severity)`、`rolling_std(severity)`、`rolling_avg(reward)`、`CUSUM` 变化点指标等

### D4. 插入“新RL算法”的改动入口在哪里？

**核心入口：**`codes/dynamic_RL34959.py`（SB3 创建模型的分支）  
你可以：
- 继续基于 SB3/SB3-Contrib（如 RecurrentPPO）做改造
- 或写自定义训练 loop（但要复用现有 `coordinationEnv` + 共享表通信机制）

同时需要同步：
- `codes/Dynamic_master34959.py`：CLI 参数 `--algorithm` 的合法值校验（目前仅允许 DQN/PPO/A2C）
- `codes/Dynamic_ALNS_RL34959.py`：`RL_ALGORITHM` 环境变量/全局变量传递
- 批量脚本（如 `codes/run_experiments_local.py`）的算法列表（如有硬编码）

### D5. 一致性/公平性建议（写论文时很关键）

如果你要在论文里说“我们的新算法更适应漂移”，建议提前定义对比规则：
- **对比 1：同信息集对比**（所有算法都能访问同样的漂移特征）  
- **对比 2：信息受限对比**（都不访问 oracle 标签，考察自检测能力）  

否则老师可能会追问：“你让算法看到了 phase_label/gt_mean，是不是相当于开卷考试？”

---

## (E) 性能与瓶颈（为并发与部署做准备）

### E1. 单步 `step()` 平均耗时怎么估计？

建议用 `rl_trace.csv` 的 `ts + stage` 做统计：只取 `stage == "receive_reward"` 的行，计算相邻两次决策的时间差（近似“ALNS 完成一次不确定事件决策”的 wall time）。

示例（你本地可直接跑）：
```py
import pandas as pd
df = pd.read_csv("codes/logs/<run>/rl_trace.csv")
df = df[df["stage"] == "receive_reward"].sort_values("ts")
df["dt"] = df["ts"].diff()
print(df["dt"].describe())
```

### E2. CPU/GPU 使用：当前瓶颈在哪里？

**现状推断（以代码为准）：**
- RL 训练 `device='cpu'`：神经网络训练不是主要瓶颈（GPU 基本没用上）。
- 大头在 **ALNS 计算 + Excel I/O（openpyxl/pandas）**：
  - 每次决策都要读 Excel（`pd.ExcelFile` + `pd.read_excel`）
  - ALNS 过程中还会写入/更新若干结果文件（历史上出现过 `BadZipFile/EOFError/WinError 5` 等并发写导致的损坏/占用）

### E3. 并发的“可行边界”（给新算法实验的建议）

经验上你会遇到两类并发瓶颈：
1) **CPU 计算瓶颈**：ALNS 本身计算重，适合用“多进程跑多个 run”（每个 run 独立 run_dir）。
2) **文件系统瓶颈**：Excel 读写在 Windows 上对并发更敏感（文件锁/原子替换）。  
   - 因此更推荐“run 粒度并发”，而不是“单个 run 内大量线程同时写同一路径”。

---

### 附：如果我要做一个“更强的新算法”，最直接的三步清单

1) 明确你要做的是 **oracle 场景感知** 还是 **自检测漂移**（决定 obs 里要不要加入 `gt_*`）。  
2) 若要自检测：先把 `episode_length=1` 的限制解除（或引入不依赖跨步记忆的漂移指标）。  
3) 在 `codes/dynamic_RL34959.py` 插入新算法分支，并在 `Dynamic_master34959.py` 开放 `--algorithm` 入口；再用 `baseline_wait/reroute` 与 `plot_paper_figure.py` 做闭环对比。

