\documentclass[11pt]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage{ctex}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
% KL 散度算子（避免 \KL 未定义）
\newcommand{\KL}{D_{\mathrm{KL}}}
% algorithm2e 关键字（你伪代码里用到了 \KwInit 和 \KwParam）
\SetKwInput{KwInit}{Initialize}
\SetKwInput{KwParam}{Hyperparameters}

% 可选：算法环境内更紧凑一点
\setlength{\parskip}{0.6em}
\setlength{\parindent}{0pt}

\title{LB-KLAC（Algorithm 2）说明}
\author{}
\date{}

\begin{document}
\maketitle

\noindent (0) \textbf{摘要与定位}\quad
非平稳环境下的强化学习要求智能体能够在环境隐含模式变化时快速、自主地适应。
算法2：\textbf{Latent Belief KL-Regularized Actor-Critic (LB-KLAC)} 针对这一情形提出了一种基于后验信念推断的策略梯度方法。
该算法假设存在隐藏的环境上下文变量$z_t$刻画环境模式变化，智能体无法直接观测$z_t$，但可以从交互历史中推断其信念分布。
LB-KLAC 使用 \textbf{Transformer} 编码近期历史得到隐含\textbf{信念向量}$b_t$，将其作为对当前环境模式的近似后验表示，并将$b_t$并入策略/价值的输入，从而在策略更新中显式建模环境不确定性。
为保证在环境变化时策略既能快速调整又不过度震荡，算法在目标函数中加入\textbf{可学习的KL散度正则项}，惩罚相邻时间步信念分布$q_\psi(z|h_t)$的剧烈改变，并通过\textbf{可变信赖域$\delta_t$}机制约束策略更新步幅。
这种设计使策略优化受到“温和”的约束：在无明显变化时策略平稳更新，当出现突变迹象时则允许策略迅速跳转。
相比仅依赖记忆加权的算法，LB-KLAC 明确地将\textbf{贝叶斯信念推断}融入强化学习，实现对非平稳性的\textbf{快速且稳健的自适应}。
在实际工程中，该算法样本效率高、训练稳定，并提供丰富的解释性输出（如注意力权重、KL散度等）以指示何时发生环境变化及策略如何响应。
下面将分别详细阐述LB-KLAC的理论原理与实现方法。

\medskip
\noindent (1) \textbf{问题形式化（非平稳MDP与信念表示）}\quad
我们考虑一个包含\textbf{隐藏上下文}的非平稳马尔可夫决策过程(MDP)
$M=(\mathcal{S}, \mathcal{A}, P, R)$。
在每个决策时刻$t$，环境状态$s_t \in \mathcal{S}$为一个低维特征向量（例如事件的延迟容忍度、严重程度、阶段指标等），智能体可采取一个二元动作$a_t \in \mathcal{A}=\{0,1\}$（如“等待”或“重新调度”）。
随后智能体立即获得奖励$r_t \in \{0,1\}$，且该单步任务结束（每个事件被视为一个独立的episode）。
非平稳性体现为：奖励函数$R(s,a)$及状态转移在随时间变化，即存在一个\textbf{不可直接观测}的环境\textbf{模式变量}$z_t$影响着决策的效用。
上下文$z_t$在一段随机时间内保持稳定不变，当发生概念漂移时跳转到新的模式（例如由模式A切换到B，随后可能返回A或转向新模式C），导致先前最优的策略不再适用。
这一过程可被视为\textbf{分段平稳}：在每个上下文段内MDP近似静态，而段与段之间发生分布跃迁。

由于$z_t$对智能体是隐藏的，我们采用\textbf{POMDP（部分可观测MDP）}的观点来形式化问题：智能体无法直接观察$z_t$，但可以通过历史决策序列$h_t$获取关于$z_t$的间接信息。
我们令
\[
h_t = \{s_1, a_1, r_1,\, s_2, a_2, r_2,\, \dots, s_{t-1}, a_{t-1}, r_{t-1},\, s_t\},
\]
表示智能体在$t$时刻所拥有的历史观测序列，包括之前各步状态、动作和奖励（此处包括当前状态$s_t$但尚未有当前动作和奖励）。
智能体的策略记作$\pi_\theta(a_t \mid s_t, h_t)$，由参数$\theta$控制，可能利用历史$h_t$来决定在状态$s_t$下动作的选择概率。
同时价值函数$V_\phi(s_t, h_t)$以参数$\phi$近似给定历史条件下状态的价值，$Q_\phi(s_t,a_t,h_t)$近似状态-动作价值，优势函数定义为
$A_\phi(s_t,a_t,h_t)=Q_\phi(s_t,a_t,h_t)-V_\phi(s_t,h_t)$。
智能体在每一步只能观察当前状态和以往交互的记录，却无法直接得知当前所属的环境模式$z_t$，因此必须\textbf{根据历史去推断隐含的上下文}。
我们的目标是学习一个策略$\pi_\theta$，能以极低的探索代价\textbf{快速适应}环境的模式变化，从而最大化长期累积奖励。
特别地，算法需具备\textbf{训练稳定}和\textbf{样本高效}的性质，并提供一定的\textbf{可解释性}，例如通过注意力权重、KL散度、价值残差等信号指示策略如何感知并适应环境变化。

\medskip
\noindent (2) \textbf{模型结构}\quad
LB-KLAC算法采用了一个带有\textbf{Transformer编码器}的 Actor-Critic 架构，用于在每一步决策时联合推断环境隐含状态并输出相应动作。
其总体结构如图示意：首先，使用参数$\psi$的Transformer网络$g_\psi(\cdot)$对最近$H$次交互的历史序列$h_t$进行编码，输出一个\textbf{上下文信念嵌入}向量$b_t$。
这个向量可以被视为当前环境模式$z_t$后验分布$q_\psi(z_t \mid h_t)$的\textbf{隐式表示}，例如对应于该后验的一组充分统计量或参数化的均值。
直观来说，$b_t$刻画了智能体对当前所处环境的“信念”或猜测。
Transformer 编码器通过自注意力机制聚合时间序列信息，能够侧重近期与决策相关的关键事件，从而提取出隐含的上下文特征：例如，当环境刚发生变化时，最近几步的异常奖励模式将在注意力中占较大权重，使得$b_t$反映新的环境模式。

随后，\textbf{Actor（策略）网络}和\textbf{Critic（价值）网络}以$s_t$和$b_t$作为联合输入，共同决定动作和价值估计。
Actor网络（参数$\theta$）以$(s_t,b_t)$为输入输出动作概率$\pi_\theta(a_t \mid s_t,b_t)$；Critic网络（参数$\phi$）以$(s_t,b_t)$为输入估计状态价值$V_\phi(s_t,b_t)$。
由于动作空间为二元，我们可以使用一个带Sigmoid输出的单元作为Actor（或softmax两维输出，其中一维对应动作1的概率），Critic则输出一个标量价值。
Actor-Critic仍采用\textbf{异策略}结构：Actor和Critic拥有各自的参数，但\textbf{共享}Transformer编码得到的信念向量$b_t$作为输入的一部分。
这意味着Transformer提取的环境隐状态既用于策略决策，也用于价值评估，从而保证两者对当前隐含上下文具有一致的理解。
需要明确各变量和模块：
\begin{itemize}
  \item \textbf{历史序列 $h_t$}：最近$H$步的交互记录序列。为了实现高效推断，实际实现中可以使用一个长度为$H$的滑动窗口队列维护最近的$(s, a, r)$序列。在每次决策时，将最新的观测添加并移出最旧的记录，构成新的$h_t$。这样Transformer编码器处理固定长度的序列，无需从初始时刻开始累积全部历史。
  \item \textbf{信念编码器 $f_\psi$（Transformer）}：采用自回归Transformer结构，输入$h_t$序列，输出隐含上下文信念$b_t = f_\psi(h_t)$。Transformer的架构可采用$L$层多头自注意力+前馈网络。为防止信息泄漏，注意力使用\textbf{因果mask}（仅关注过去信息）。建议Transformer隐藏维度与输出维度（即$|b|$）设置为适中，例如$|b|=10\sim50$，以便有足够能力表示多种环境模式又不过拟合。头数和层数可根据历史长度调整，例如$H=50$时使用4头注意力、1--2层Transformer已能捕获主要模式。Transformer输出$b_t$既可直接作为embedding使用，也可进一步视作后验$q(z|h_t)$参数（例如作为高斯分布的均值），必要时可拓展为表示更复杂分布（如高斯混合的参数）。
  \item \textbf{Actor网络 $\pi_\theta$}：策略网络接受$(s_t,b_t)$为输入。可以将$b_t$与当前状态特征向量$s_t$级联，经由若干全连接层提取联合特征，最终输出动作概率。例如，采用两层MLP，隐层使用ReLU激活，输出层用Sigmoid/Softmax得到$\pi_\theta(a_t=1 \mid s_t,b_t)$。策略参数$\theta$通过策略梯度更新，其梯度既受环境奖励影响，也会由于$b_t$的变化而受到$\psi$的间接影响。
  \item \textbf{Critic网络 $V_\phi$}：价值网络也接受$(s_t,b_t)$为输入，结构可类似Actor的MLP但输出单一标量$V_\phi(s_t,b_t)$估计状态价值。价值网络参数$\phi$通过时序差分(TD)误差训练。因为$b_t$是以$\psi$参数生成，价值损失对$\psi$也有梯度（通过$V_\phi(s,b)$对$b$的依赖），这确保信念提取不仅关注策略优化，也兼顾价值预测的准确性。
\end{itemize}

上述结构实现了一个基于隐含\textbf{belief状态}$b_t$的增强状态空间$\tilde{s}_t=(s_t,b_t)$。
从理论上讲，如果Transformer能够精确地提取出真实的环境模式（即$b_t$充分刻画$z_t$），则$\tilde{s}_t$对智能体而言就是完全可观测的，此时环境转化为\textbf{静态MDP}。
策略$\pi(a|s,b)$针对的是该等价MDP，能取得与知晓真实上下文时几乎相同的性能。
这为我们针对非平稳环境引入隐变量推断的策略提供了理论基础——智能体通过内部维护的信念将问题转化为一个随时间演化的后验估计，从而在\textbf{belief-MDP}上应用标准强化学习方法。

\medskip
\noindent (3) \textbf{训练目标与推导}\quad
LB-KLAC的训练目标在标准Actor-Critic目标上加入了\textbf{信念后验建模和正则化}项，可视为一种带约束的证据下界(ELBO)优化。
具体地，策略梯度部分遵循常规的优势函数策略优化，而信念推断部分通过KL散度项与前一步保持连续性。
综合考虑，我们在每个时间步$t$定义以下\textbf{总损失函数}并对其进行最小化：
\[
L_{\text{LB}}(\theta,\phi,\psi) \;=\; L_{\pi}(\theta,\psi)\;+\;c_V\,L_V(\phi,\psi)\;+\;\beta\,D_{\text{KL}}\!\Big(q_{\psi}(z \mid h_t)\,\Big\Vert\,q_{\psi}(z \mid h_{t-1})\Big)\;+\;c_H\,L_H(\theta,\psi)~,
\]
其中各部分含义如下：
\begin{itemize}
  \item \textbf{策略损失 $L_{\pi}(\theta,\psi)$}：采用策略梯度形式，$L_{\pi} = -\mathbb{E}_t\big[\log \pi_\theta(a_t \mid s_t,b_t)\,A_t\big]$。在实现时，我们使用$A_t$为优势估计，如采用单步TD优势$\hat{A}_t = r_t + \gamma V_\phi(s_{t+1},b_{t+1}) - V_\phi(s_t,b_t)$，或使用广义优势估计(GAE)累积多步优势。策略梯度的梯度$\nabla_\theta L_\pi = -\mathbb{E}[\nabla_\theta \log\pi_\theta A_t]$引导提高对优势为正的动作概率、降低优势为负的动作概率，从而提升性能。需要注意$\psi$参数也出现在$\pi_\theta(\cdot|s,b)$的输入$b_t$中，因此该项对$\psi$亦有梯度信号，这意味着信念提取网络将被优化以产生对获得高回报有利的$b_t$表示。
  \item \textbf{价值损失 $L_V(\phi,\psi)$}：采用均方误差形式，$L_V = \mathbb{E}_t\big[\frac{1}{2}(V_\phi(s_t,b_t) - R_t^{\text{target}})^2\big]$，其中$R_t^{\text{target}}$是价值的目标回报。对于单步任务，可取$R_t^{\text{target}} = r_t$（无折扣，一步结束）或包含下一状态值以降低方差：$R_t^{\text{target}} = r_t + \gamma V_{\phi_{\text{old}}}(s_{t+1},b_{t+1})$。我们在实现中采用后者进行bootstrap，以提升价值估计稳定性。$\phi$的梯度来自时序差分误差$(R_t^{\text{target}}-V_\phi)^2$，而$\psi$参数也通过$V_\phi(s,b)$的输入获得更新（鼓励生成能更准确预测奖励的信念$b_t$）。
  \item \textbf{信念平滑正则 $D_{\text{KL}}(q_\psi(z|h_t)\Vert q_\psi(z|h_{t-1}))$}：这是算法的核心新增项。它以KL散度衡量相邻时间步骤推断出的隐含上下文分布$q_t$和$q_{t-1}$之间的差异。该项通过系数$\beta$加权加入总损失中，其中$\beta>0$是预设的正则强度超参数。优化目标包含$\beta\,D_{\text{KL}}$项意味着：\textbf{若当前信念与上一时刻相比变化剧烈}，则损失增大，从而在梯度下降中产生一种约束效应，迫使$\psi$（以及通过$b_t$影响的$\theta,\phi$）的更新倾向于\textbf{减小信念变动}。反之，如果信念改变很小（表示策略认为环境基本延续之前的模式），该项损失接近0，则训练主要根据策略/价值损失来更新。这相当于为模型提供了一种“惯性”：除非有强烈的证据表明环境发生改变，否则不要轻易放弃先前的信念。这一机制能够有效防止智能体对单次随机波动作出过度反应（\textbf{误警}），从而提升训练的稳定性。同时，当真实环境已经改变且新证据充足时，策略依然可以通过累积的优势信号推动信念$b_t$发生较大更新，只是过程会较为“平滑”而非骤变。换言之，KL正则项提供了对策略和信念更新的\textbf{锚定}(anchoring)作用，使智能体在非平稳环境中避免灾难性遗忘：过去学到的有用策略不会被新数据迅速覆盖，一旦环境返回旧模式，智能体仅需微调信念即可恢复先前策略。
  \item \textbf{熵奖励 $L_H(\theta,\psi)$}：为避免探索不足，我们可加入策略熵的惩罚项$L_H = -\mathbb{E}_t[\mathcal{H}(\pi_\theta(\cdot|s_t,b_t))]$，即鼓励策略的熵增大。在实现中，这通常以权重$c_H$（例如0.01左右）乘以负熵加到总损失中。其效果是使策略在未充分确定最优动作时保持一定的随机性，从而更有机会探索新的动作输出组合。注意由于$\psi$影响策略输出的概率分布，该熵项同样对$\psi$有梯度，鼓励产生多样性更大的信念$b_t$。
\end{itemize}

有了上述各项，\textbf{总损失} $L_{\text{LB}}$ 综合了策略优化、价值估计、信念约束和探索需求。
我们对$\theta,\phi,\psi$求解$\min L_{\text{LB}}$，常用的方法是随机梯度下降（如Adam优化器）。
因为$\psi$贯穿于$\pi$和$V$的输入，梯度传播时$\psi$将同时受到\textbf{回报驱动}（通过$L_\pi,L_V$）和\textbf{平滑驱动}（通过$D_{\text{KL}}$）的影响。
这种联合更新实现了策略更新和上下文推断的\textbf{协同演化}：$\psi$既被奖励信号引导去辨别能够解释高奖励的上下文变化，也被正则项约束避免无根据的大幅波动。
\textbf{直观解释}：这类似于贝叶斯元学习中的后验更新，只不过将其融入策略梯度框架。
$D_{\text{KL}}(q_t\Vert q_{t-1})$惩罚过大的“belief惊异度”，使模型在没有强证据时保持对过去模式的记忆，从而当环境往复变化(例如模式A$\to$B$\to$A)时能够快速返回旧模式的策略。
理论上，若$\psi$可以逼近真实的环境后验，则在最优情况下$D_{\text{KL}}(q_t\Vert q_{t-1})$只在环境真正变化时才会增大，否则保持很小。
此时，LB-KLAC的更新等价于在\textbf{已知上下文}的MDP上进行策略迭代，其性能接近于“先知”策略。
在上下文变化不过于频繁（未超出模型表达能力）的前提下，LB-KLAC 的收敛特性与标准A2C/PPO在静态环境中的收敛类似，但在非平稳场景下能够取得远优于假设环境不变算法的表现。

需要注意的是，在实现中我们通常还结合\textbf{PPO截断技巧}保证策略更新的数值稳定。
也就是说，在策略损失$L_\pi$中，对$\log\pi_\theta(a|s,b)$项采用PPO的剪切概率比$r_t(\theta)$，并在更新过程中监控$\pi$的新旧分布KL散度，以防止单次更新跨越过大。
LB-KLAC本身鼓励小步更新（通过$\delta_t$约束，见下节），这与PPO的思想相辅相成：正则项控制信念和策略缓慢演化，而截断机制避免梯度估计偏差过大，因此可以使用稍高的学习率而保持稳定。
总之，策略梯度、KL正则和截断/熵等技术相结合，使LB-KLAC在非平稳环境中实现\textbf{稳定高效}的训练。

\medskip
\noindent (4) \textbf{可变信赖域 $\delta_t$ 机制}\quad
除了上述KL正则对信念演化进行约束外，LB-KLAC还通过一个\textbf{随时间自适应}的策略信赖域半径$\delta_t$对每次策略更新的幅度施加直接限制。
\textbf{信赖域}的思想来源于信赖域策略优化(TRPO)：要求新旧策略在每个状态上的分布差异不能超过阈值$\delta$，即
$D_{\text{KL}}(\pi_{\theta_{\text{new}}}(\cdot|s,b)\,\Vert\,\pi_{\theta_{\text{old}}}(\cdot|s,b)) \le \delta$。
这确保策略参数更新不会偏离旧策略过远，从而保证策略单调改进和稳定性。
然而，在非平稳环境中，不同时间应允许\textbf{不同大小}的策略步长：环境稳定时，应保持$\delta_t$较小以免过度调整；环境突变时，则需要临时放宽$\delta_t$以\textbf{加快适应}。
LB-KLAC通过以下两种途径实现$\delta_t$的自适应调节：

\textbf{途径一：基于启发的动态调整。}
我们利用智能体的\textbf{预测误差}或\textbf{突变指标}来判断当前环境是否发生变化，并相应放宽或收紧信赖域。
一种简单而有效的指标是\textbf{价值残差}$\varepsilon_t$，即当前奖励与Critic先验估计的差值：
$\varepsilon_t = \big|r_t - V_{\phi_{\text{old}}}(s_t, b_t)\big|$。
当$\varepsilon_t$显著偏大时，表示在状态$s_t$下实际得到的奖励与旧价值预测差异很大，暗示环境动力学或奖励规律可能发生了改变——策略对当前情境“措手不及”。
因此，我们认为出现\textbf{潜在漂移}，需要更大的策略更新步长快速追踪变化。
反之，若$\varepsilon_t$很小，说明环境与策略预测基本一致，没有明显新信息，则维持一个小步长以稳健优化。
基于此，我们设定阈值$\varepsilon_{\text{OOD}}$（Out-of-Distribution阈值）检测异常，当$\varepsilon_t$超过阈值时判定发生\textbf{漂移警报}，动态调整$\delta_t$：
\[
\delta_t \;=\;
\begin{cases}
\min(\delta_{\max},\; \kappa_{\uparrow}\,\delta_{t-1})~, & \text{若 }\varepsilon_t > \varepsilon_{\text{OOD}} \text{（出现异常差异）},\\[1ex]
\max(\delta_{\min},\; \kappa_{\downarrow}\,\delta_{t-1})~, & \text{若 }\varepsilon_t \le \varepsilon_{\text{OOD}} \text{（无显著异常）}.
\end{cases}
\]
这里$\kappa_{\uparrow}>1$是扩张系数（如1.5），$\kappa_{\downarrow}<1$是收缩系数（如0.9），$\delta_{\max}$和$\delta_{\min}$分别是信赖域半径的上下限，用于防止$\delta_t$无限放大或缩小。
通过上述规则，$\delta_t$会根据环境变化程度进行\textbf{几何型}调整：当检测到可能的环境变化时，$\delta_t$乘以$\kappa_{\uparrow}$放宽（例如从0.1增至0.15），允许接下来策略有更大的更新幅度以适应新情况；当环境平稳时，$\delta_t$逐步乘以$\kappa_{\downarrow}$收紧（如从0.15降至0.135），使策略更新更谨慎。
整个调整是平滑连续的，且只有在检测到持续异常时才会多次放宽，避免了单步噪声导致的$\delta$剧烈抖动。
此外，$\varepsilon_{\text{OOD}}$的设定应结合环境噪声水平选择，可通过监控价值残差的均值和方差来设定，例如以残差滚动平均和标准差计算一个动态阈值，保证只有在置信度较高的情况下才触发策略“大步走”。

上述基于价值残差的方法是一种不直接依赖环境真值的\textbf{无监督变化检测}：当环境发生概念漂移时，往往会表现为策略的预测误差突然增大，因此这是一个有效的启发。
在LB-KLAC实现中，我们也可以选择其他指标，如\textbf{信念KL增量}：即直接以
$D_{\text{KL}}(q_\psi(z|h_t)\Vert q_\psi(z|h_{t-1}))$
的大小作为变化指示。
如果该KL值在某一步骤骤增，意味着新接收到的观测与旧信念严重不符，也暗示环境可能变换。
这种方法无需Critic，但由于$q_\psi$本身受KL正则影响，其变化滞后性更强，实践中可以和价值残差结合使用，提高检测的鲁棒性。

\textbf{途径二：基于元学习的策略。}
除了启发式规则，$\delta_t$还可以作为一个需学习的参数，由\textbf{元梯度}自动调节。
具体而言，我们将信赖域半径的选择视为影响策略长期回报的高阶决策。
定义$J(\theta(\cdot;\delta))$为在信赖域参数$\delta$控制下经过训练得到的策略性能评价（例如整个训练过程的累积奖励或最终测试奖励），希望直接优化$\delta$以最大化$J$。
由于$\delta$影响每次策略更新的幅度，其对最终绩效的作用是通过整个训练轨迹体现的，因而$\nabla_{\delta} J$被称为元梯度（meta-gradient）。
在理论上，我们可以通过对训练过程进行微分来得到这个梯度，并采用梯度上升更新$\delta$。
实际实现中，由于对完整训练过程求导的计算和存储开销巨大，常用近似方法：例如每隔一定更新周期评估不同$\delta$对短期性能的影响，或者只对有限步的近似上升方向进行估计。
一个思路是在实现中将$\delta$设为可训练变量，将每次策略更新视为依赖$\delta$的运算，利用自动微分框架累积若干步的梯度后，对$\delta$执行一次更新。
也可以采用\textbf{多臂Bandit}或\textbf{强化学习}的方法调整$\delta$：例如在每个新环境段开始时，根据上一个段的适应表现选择更大或更小的$\delta$。
需要注意元学习$\delta$易受噪声影响，我们可以结合策略稳定性约束，确保$\delta$的学习不会本身引入过大不稳定。

无论采用何种途径，我们都会将\textbf{信赖域约束}应用于策略参数更新中。
在实现中，这通常有两种方式：(i) \textbf{显式投影}：在每次参数更新前后计算新旧策略的KL散度，若超过$\delta_t$则缩小步长，直到满足约束；
(ii) \textbf{惩罚项近似}：在损失函数中增加$\lambda\,D_{\text{KL}}(\pi_{\theta_{\text{new}}}\Vert\pi_{\theta_{\text{old}}})$的惩罚，以较大的$\lambda$迫使最优解满足近似约束。
这两者在小步长情况下等价。
我们在LB-KLAC中采用方法(i)：即预先设定$\delta_t$，尝试一次基于梯度的更新，\textbf{评估} $\KL(\pi_{\theta_{\text{new}}}\Vert\pi_{\theta_{\text{old}}})$，
如果超过$\delta_t$则按比例缩减梯度步长$\alpha$（或者进行简化的\textbf{回退}/Line Search）。
这一过程确保了每次策略更新的KL变化被控制在阈值内。
这种机制配合$\delta_t$的自适应调整，使得算法能在需要时灵活加大步长，又在平常时严格限制步长，相当于一个\textbf{安全网}，保障训练不因为瞬时异常而发散。

\medskip
\noindent (5) \textbf{变化检测与解释性输出}\quad
LB-KLAC算法不仅通过内部机制对变化做出响应，还在输出上提供对环境变化的指示，方便监控和解释智能体行为。

首先，\textbf{变化检测}可借助算法内部若干指标实现：
\begin{itemize}
  \item \textbf{信念KL散度}：$\Delta_t = D_{\text{KL}}(q_\psi(z|h_t)\Vert q_\psi(z|h_{t-1}))$直接量化了智能体对于当前环境与前一步相比的认知变化程度。如果$\Delta_t$显著升高，说明新观测让智能体显著修正了对环境的信念，很可能意味着环境发生了改变。因此$\Delta_t$可以作为变化检测的灵敏指标。在实践中，为了降低噪声带来的误报，可对$\Delta_t$做平滑处理，如取最近$k$步的平均KL或中位数，当其持续高于某阈值时再判断发生真正漂移。
  \item \textbf{价值残差/预测误差}：$\varepsilon_t = |r_t - V_{\phi_{\text{old}}}(s_t,b_t)|$同样是重要信号。当$\varepsilon_t$连续异常偏高时，意味着以当前策略评估环境出现了系统偏差。前述$\delta_t$调整已经利用该信号来触发大步更新，我们也可将其用于监控。当$\varepsilon_t$在多个时间步维持高位，可以认定环境分布已变迁。
  \item \textbf{奖励统计}：对最近窗口的平均奖励或成功率进行统计检测。例如，可采用累积和检验(CUSUM)或Page-Hinkley检测来判断奖励分布是否产生结构性突变。这些统计方法可以在较低假警率下检测出分布均值的转变，结合LB-KLAC内部信号能更加可靠地确定变化点。
  \item \textbf{策略分布变化}：虽然有$\delta_t$约束，但若环境实质变化，新策略与旧策略仍会产生一定差异。监控$\KL(\pi_{\theta_t}\Vert \pi_{\theta_{t-1}})$或策略熵等，也能补充反映策略调整的幅度。当策略对某一动作的概率发生突跳且这种改变与环境模式已知变化吻合时，说明算法正确地捕捉并应对了变化。
\end{itemize}

综合以上指标，我们可以实现一套稳健的\textbf{变化检测系统}。
工程上建议对关键指标进行日志记录和可视化：例如，将每次决策的$\Delta_t$、$\varepsilon_t$、$\pi$熵值等写入日志（如\texttt{rl\_trace.csv}的附加列），并在训练过程中实时绘制曲线，辅助研究者判断算法何时“察觉”到了环境变化。
如果需要进一步减少误警，可设置\textbf{双阈值滞后}：如要求$\Delta_t$连续$n$步超过阈值才判定变化开始，低于另一较低阈值$m$步才判定变化结束，避免边界噪声反复横跳。

其次，LB-KLAC提供了丰富的\textbf{可解释性输出}，使我们能够洞察策略决策依据：
\begin{itemize}
  \item \textbf{注意力权重}：Transformer编码器的自注意力机制使得隐含信念$b_t$是由历史序列不同部分加权得到的。通过提取Transformer最后一层对各历史输入的注意力得分，我们可以知道智能体在形成当前信念时“关注”了哪些过去事件。例如，在环境发生改变后，Transformer往往会对变化发生后的几步观测赋予最高权重，而忽略更早之前的过时信息。这与我们期望的“关注近期关键转变”相符，因此注意力模式可以作为策略依据的直观解释。我们可以将注意力权重随时间绘制成热力图，观察在每个时刻$t$，模型将注意力集中在哪几个过去时刻。当注意力焦点突然前移（从过去远处跳到最近处），往往对应环境刚发生一次变化。研究者也可检查这些被关注的历史事件的具体内容（状态、动作、奖励），理解智能体因何调整了策略。
  \item \textbf{信念嵌入/上下文ID}：虽然$b_t$本身是一个连续向量，但我们可以对其进行降维或聚类以提取更高层次的\textbf{上下文标识}。例如，采用t-SNE或PCA将所有记录的$b_t$投影到二维平面，可以观察它们是否分成几簇，每簇对应一种环境隐模式。如果环境实际模式种类有限（如A/B/C三种），我们预期$b_t$将自适应地聚类成对应的三团。在有监督情况下（仿真环境我们知道每步的真实phase标签），我们甚至可以给每个$b_t$打上真值标签，验证Transformer是否学出了正确的后验。工程实现中，可增设一个\texttt{context\_id}字段，将当前信念向量所属的簇编号记录下来。这样在日志中我们可直接看到算法判定的“当前上下文ID”，并对比真实环境模式ID以评估准确性。在应用中，即使未知真值，我们也可通过context\_id的变化频率来估计环境变化：若context\_id长时间稳定，说明算法认为处于同一模式；若频繁跳变，则可能算法过敏或环境极不稳定。
  \item \textbf{漂移评分}：结合信念变化和价值残差，我们可以定义一个\textbf{综合漂移评分}（drift score），例如$\text{score}_t = \frac{\Delta_t}{\Delta_t + 1} \cdot \min(1, \frac{\varepsilon_t}{\varepsilon_0})$（这里只是示例，可以设计成0--1范围的无量纲量）。这样的评分在环境无变化时接近0，有变化时接近1，可记录在日志并随时间绘制，从而实时指示环境漂移的程度。我们的实现中，已在环境步骤的info或trace日志中增加了\texttt{drift\_score}字段留作扩展。
  \item \textbf{价值函数残差分析}：价值网络的预测误差除了用于变化检测，也可帮助解释\textbf{性能损失}的来源。如果在某段时间内$\varepsilon_t$持续为正（实际奖励高于预测），意味着策略低估了当前环境的可获奖励，往往对应策略尚未充分开发新的最优动作；反之若$\varepsilon_t$持续为负，则策略过于乐观，可能在执行某动作时经常拿不到预期的奖励。这些信息提示我们在哪些情境下策略可能需要调整。此外，监控\textbf{优势分布}也有助解释决策：当环境变化时，旧策略在新的状态下优势分布可能偏负（许多动作都比估计的差），这是促使策略改变的直接原因。
\end{itemize}

通过以上多角度的解释性输出，我们可以\textbf{重构智能体的决策过程}：从注意力看到它参考了哪些历史事件，从信念向量或上下文ID判断其对环境所处模式的理解，从漂移评分判断环境变化强度，从价值残差了解策略预测偏差。
这样不仅有助于开发者调试和改进算法，也增强了算法的\textbf{透明度}，便于向终端用户展示“算法为何这么做”。
尤其在实际调度系统中，这种可解释性非常重要——运营人员希望了解AI决策背后的依据，以建立信任并在异常时进行人为干预。
LB-KLAC通过内置的Transformer注意力和Bayesian信念，使这一点成为可能。

\medskip
\noindent (6) \textbf{伪代码}\quad
下面给出LB-KLAC算法的训练与决策流程伪代码（Algorithm 2）。
该伪代码涵盖了从环境交互到参数更新的完整过程，包括经验缓存和信赖域控制等实现细节。

\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Latent Belief KL-Regularized Actor-Critic (LB-KLAC)}
\KwInit{Initialize policy parameters $\theta$, value parameters $\phi$, inference parameters $\psi$; Initialize (optional) trust-region radius $\delta$ (e.g. $\delta=0.1$)}
\KwInit{Initialize empty replay buffer $\mathcal{D}$}
\KwParam{Hyperparameters: KL regularization weight $\beta$, initial trust-region $\delta$, trust region bounds $\delta_{\min},\delta_{\max}$, adjust factors $\kappa_{\uparrow},\kappa_{\downarrow}$, surprise threshold $\varepsilon_{\text{OOD}}$, batch size $B$, update interval $N_{\text{update}}$, learning rate $\alpha$, discount $\gamma$}
\BlankLine
\For{each incoming uncertain event (time step $t$)}{
  Observe current state $s_t$ \;
  Obtain history $h_t$ (sequence of last $H$ transitions up to $s_t$) \;
  Compute belief embedding $b_t = f_\psi(h_t)$ \tcp*{Transformer inference of context}
  Select action $a_t \sim \pi_\theta(\cdot \mid s_t,\,b_t)$ \tcp*{Sample from current policy (could use $\epsilon$-greedy or softmax exploration)}
  Execute action $a_t$ in environment \;
  Observe reward $r_t$ and next state $s_{t+1}$ \;
  Store transition $(s_t,\; b_t,\; a_t,\; r_t,\; s_{t+1})$ into buffer $\mathcal{D}$ \;
  \BlankLine
  \tcp*[l]{Optional: online change detection and trust-region adaptation}
  Compute prediction error $e_t = \big| r_t - V_{\phi}(s_t, b_t) \big|$ \;
  \uIf{$e_t > \varepsilon_{\text{OOD}}$ {\bf or} $D_{\text{KL}}\!\big(q_\psi(z|h_t)\,\Vert\,q_\psi(z|h_{t-1})\big) >$ high threshold}{
      $\delta \leftarrow \min(\delta_{\max},\; \kappa_{\uparrow}\,\delta)$  \tcp*{loosen trust-region radius}
  }
  \Else{
      $\delta \leftarrow \max(\delta_{\min},\; \kappa_{\downarrow}\,\delta)$  \tcp*{tighten trust-region radius}
  }
  \BlankLine
  \tcp*[l]{Periodic policy/value update}
  \If(\tcp*[f]{update every $N_{\text{update}}$ transitions}){$t$ mod $N_{\text{update}} == 0$}{
    Sample mini-batch $\mathcal{B}$ of transitions from $\mathcal{D}$ (prioritize most recent) \;
    \ForEach{transition $(s_i, b_i, a_i, r_i, s_i')$ in $\mathcal{B}$}{
       Compute $b_i' = f_\psi(h_{i+1})$ for next state $s_i'$ (using updated history) \;
       Compute advantage $\displaystyle \hat{A}_i = r_i + \gamma\,V_{\phi}(s_i', b_i') - V_{\phi}(s_i, b_i)$ \;
       $L_{\pi} \mathrel{+}= -\log \pi_{\theta}(a_i \mid s_i, b_i)\;\hat{A}_i$ \tcp*{policy gradient loss (PPO clipping can be applied here)}
       $L_{V} \mathrel{+}= \frac{1}{2}\Big(V_{\phi}(s_i, b_i) - \big(r_i + \gamma\,V_{\phi}(s_i', b_i')\big)\Big)^2$ \tcp*{value TD loss}
       $L_{\text{KL}} \mathrel{+}= D_{\text{KL}}\!\Big(q_\psi(z \mid h_i)\,\Vert\,q_\psi(z \mid h_{i-1})\Big)$ \tcp*{belief smoothness loss}
       $L_{H} \mathrel{+}= -\mathcal{H}\!\big(\pi_\theta(\cdot \mid s_i, b_i)\big)$ \tcp*{entropy bonus (optional)}
    }
    Normalize losses: $L_{\pi} \mathrel{/=} |\mathcal{B}|$, $L_{V} \mathrel{/=} |\mathcal{B}|$, $L_{\text{KL}} \mathrel{/=} |\mathcal{B}|$ (and $L_H$ similarly) \;
    Compute total loss $L_{\text{total}} = L_{\pi} + c_V L_{V} + \beta\,L_{\text{KL}} + c_H L_{H}$ \;
    Compute gradients $(\nabla_\theta,\nabla_\phi,\nabla_\psi) = \nabla_{\theta,\phi,\psi} L_{\text{total}}$ \;
    Propose parameter update: $(\theta',\phi',\psi') = (\theta,\phi,\psi) - \alpha\,(\nabla_\theta,\nabla_\phi,\nabla_\psi)$ \;
    Evaluate policy divergence $\displaystyle \Delta_{\pi} = D_{\text{KL}}\!\Big(\pi_{\theta'}(\cdot \mid s, b)\;\Big\Vert\;\pi_{\theta}(\cdot \mid s, b)\Big)$ averaged over batch states \;
    \If(\tcp*[f]{ensure policy update within trust region}){$\Delta_{\pi} > \delta$}{
       Scale gradient: $(\nabla_\theta,\nabla_\phi,\nabla_\psi) \leftarrow \frac{\delta}{\Delta_{\pi}}\,(\nabla_\theta,\nabla_\phi,\nabla_\psi)$ \tcp*{reduce step size proportionally}
       Update $(\theta',\phi',\psi') = (\theta,\phi,\psi) - \alpha\,(\nabla_\theta,\nabla_\phi,\nabla_\psi)$ \;
    }
    Commit parameter update: $(\theta,\phi,\psi) \leftarrow (\theta',\phi',\psi')$ \;
  }
}
\end{algorithm}

\end{document}
