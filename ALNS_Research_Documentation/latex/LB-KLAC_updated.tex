\documentclass[11pt]{article}

\usepackage[a4paper,margin=1in]{geometry}
\usepackage{ctex}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
% KL 散度算子（避免 \KL 未定义）
\newcommand{\KL}{D_{\mathrm{KL}}}
% algorithm2e 关键字（你伪代码里用到了 \KwInit 和 \KwParam）
\SetKwInput{KwInit}{Initialize}
\SetKwInput{KwParam}{Hyperparameters}

% 可选：算法环境内更紧凑一点
\setlength{\parskip}{0.6em}
\setlength{\parindent}{0pt}

\title{LB-KLAC（Algorithm 2）说明}
\author{}
\date{}

\begin{document}
\maketitle

\noindent (0) \textbf{摘要与定位}\quad
非平稳环境下的强化学习要求智能体能够在环境隐含模式变化时快速、自主地适应。
算法2：\textbf{Latent Belief KL-Regularized Actor-Critic (LB-KLAC)} 针对这一情形提出了一种基于后验信念推断的策略梯度方法。
该算法假设存在隐藏的环境上下文变量$z_t$刻画环境模式变化，智能体无法直接观测$z_t$，但可以从交互历史中推断其信念分布。
LB-KLAC 使用 \textbf{Transformer} 编码近期历史得到隐含\textbf{信念向量}$b_t$，将其作为对当前环境模式的近似后验表示，并将$b_t$并入策略/价值的输入，从而在策略更新中显式建模环境不确定性。
为保证在环境变化时策略既能快速调整又不过度震荡，算法在目标函数中加入\textbf{可学习的KL散度正则项}，惩罚相邻时间步信念分布$q_\psi(z|h_t)$的剧烈改变，并通过\textbf{可变信赖域$\delta_t$}机制约束策略更新步幅。
这种设计使策略优化受到“温和”的约束：在无明显变化时策略平稳更新，当出现突变迹象时则允许策略迅速跳转。
相比仅依赖记忆加权的算法，LB-KLAC 明确地将\textbf{贝叶斯信念推断}融入强化学习，实现对非平稳性的\textbf{快速且稳健的自适应}。
在实际工程中，该算法样本效率高、训练稳定，并提供丰富的解释性输出（如注意力权重、KL散度等）以指示何时发生环境变化及策略如何响应。
下面将分别详细阐述LB-KLAC的理论原理与实现方法。

\medskip
\noindent (1) \textbf{问题形式化（非平稳MDP与信念表示）}\quad
我们考虑一个包含\textbf{隐藏上下文}的非平稳马尔可夫决策过程(MDP)
$M=(\mathcal{S}, \mathcal{A}, P, R)$。
在每个决策时刻$t$，环境状态$s_t \in \mathcal{S}$为一个低维特征向量（例如事件的延迟容忍度、严重程度、阶段指标等），智能体可采取一个二元动作$a_t \in \mathcal{A}=\{0,1\}$（如“等待”或“重新调度”）。
随后智能体立即获得奖励$r_t \in \{0,1\}$，且该单步任务结束（每个事件被视为一个独立的episode）。
非平稳性体现为：奖励函数$R(s,a)$及状态转移在随时间变化，即存在一个\textbf{不可直接观测}的环境\textbf{模式变量}$z_t$影响着决策的效用。
上下文$z_t$在一段随机时间内保持稳定不变，当发生概念漂移时跳转到新的模式（例如由模式A切换到B，随后可能返回A或转向新模式C），导致先前最优的策略不再适用。
这一过程可被视为\textbf{分段平稳}：在每个上下文段内MDP近似静态，而段与段之间发生分布跃迁。

由于$z_t$对智能体是隐藏的，我们采用\textbf{POMDP（部分可观测MDP）}的观点来形式化问题：智能体无法直接观察$z_t$，但可以通过历史决策序列$h_t$获取关于$z_t$的间接信息。
我们令
\[
h_t = \{s_1, a_1, r_1,\, s_2, a_2, r_2,\, \dots, s_{t-1}, a_{t-1}, r_{t-1},\, s_t\},
\]
表示智能体在$t$时刻所拥有的历史观测序列，包括之前各步状态、动作和奖励（此处包括当前状态$s_t$但尚未有当前动作和奖励）。
智能体的策略记作$\pi_\theta(a_t \mid s_t, h_t)$，由参数$\theta$控制，可能利用历史$h_t$来决定在状态$s_t$下动作的选择概率。
同时价值函数$V_\phi(s_t, h_t)$以参数$\phi$近似给定历史条件下状态的价值，$Q_\phi(s_t,a_t,h_t)$近似状态-动作价值，优势函数定义为
$A_\phi(s_t,a_t,h_t)=Q_\phi(s_t,a_t,h_t)-V_\phi(s_t,h_t)$。
智能体在每一步只能观察当前状态和以往交互的记录，却无法直接得知当前所属的环境模式$z_t$，因此必须\textbf{根据历史去推断隐含的上下文}。
我们的目标是学习一个策略$\pi_\theta$，能以极低的探索代价\textbf{快速适应}环境的模式变化，从而最大化长期累积奖励。
特别地，算法需具备\textbf{训练稳定}和\textbf{样本高效}的性质，并提供一定的\textbf{可解释性}，例如通过注意力权重、KL散度、价值残差等信号指示策略如何感知并适应环境变化。

\medskip
\noindent (2) \textbf{模型结构}\quad
LB-KLAC算法采用了一个带有\textbf{Transformer编码器}的 Actor-Critic 架构，用于在每一步决策时联合推断环境隐含状态并输出相应动作。
其总体结构如图示意：首先，使用参数$\psi$的Transformer网络$g_\psi(\cdot)$对最近$H$次交互的历史序列$h_t$进行编码，输出一个\textbf{上下文信念嵌入}向量$b_t$。
这个向量可以被视为当前环境模式$z_t$后验分布$q_\psi(z_t \mid h_t)$的\textbf{隐式表示}，例如对应于该后验的一组充分统计量或参数化的均值。
直观来说，$b_t$刻画了智能体对当前所处环境的“信念”或猜测。
Transformer 编码器通过自注意力机制聚合时间序列信息，能够侧重近期与决策相关的关键事件，从而提取出隐含的上下文特征：例如，当环境刚发生变化时，最近几步的异常奖励模式将在注意力中占较大权重，使得$b_t$反映新的环境模式。

随后，\textbf{Actor（策略）网络}和\textbf{Critic（价值）网络}以$s_t$和$b_t$作为联合输入，共同决定动作和价值估计。
Actor网络（参数$\theta$）以$(s_t,b_t)$为输入输出动作概率$\pi_\theta(a_t \mid s_t,b_t)$；Critic网络（参数$\phi$）以$(s_t,b_t)$为输入估计状态价值$V_\phi(s_t,b_t)$。
由于动作空间为二元，我们可以使用一个带Sigmoid输出的单元作为Actor（或softmax两维输出，其中一维对应动作1的概率），Critic则输出一个标量价值。
Actor-Critic仍采用\textbf{异策略}结构：Actor和Critic拥有各自的参数，但\textbf{共享}Transformer编码得到的信念向量$b_t$作为输入的一部分。
这意味着Transformer提取的环境隐状态既用于策略决策，也用于价值评估，从而保证两者对当前隐含上下文具有一致的理解。
需要明确各变量和模块：
\begin{itemize}
  \item \textbf{历史序列 $h_t$}：最近$H$步的交互记录序列。为了实现高效推断，实际实现中可以使用一个长度为$H$的滑动窗口队列维护最近的$(s, a, r)$序列。在每次决策时，将最新的观测添加并移出最旧的记录，构成新的$h_t$。这样Transformer编码器处理固定长度的序列，无需从初始时刻开始累积全部历史。
  \item \textbf{信念编码器 $f_\psi$（Transformer）}：采用自回归Transformer结构，输入$h_t$序列，输出隐含上下文信念$b_t = f_\psi(h_t)$。Transformer的架构可采用$L$层多头自注意力+前馈网络。为防止信息泄漏，注意力使用\textbf{因果mask}（仅关注过去信息）。建议Transformer隐藏维度与输出维度（即$|b|$）设置为适中，例如$|b|=10\sim50$，以便有足够能力表示多种环境模式又不过拟合。头数和层数可根据历史长度调整，例如$H=50$时使用4头注意力、1--2层Transformer已能捕获主要模式。Transformer输出$b_t$既可直接作为embedding使用，也可进一步视作后验$q(z|h_t)$参数（例如作为高斯分布的均值），必要时可拓展为表示更复杂分布（如高斯混合的参数）。
  \item \textbf{Actor网络 $\pi_\theta$}：策略网络接受$(s_t,b_t)$为输入。可以将$b_t$与当前状态特征向量$s_t$级联，经由若干全连接层提取联合特征，最终输出动作概率。例如，采用两层MLP，隐层使用ReLU激活，输出层用Sigmoid/Softmax得到$\pi_\theta(a_t=1 \mid s_t,b_t)$。策略参数$\theta$通过策略梯度更新，其梯度既受环境奖励影响，也会由于$b_t$的变化而受到$\psi$的间接影响。
  \item \textbf{Critic网络 $V_\phi$}：价值网络也接受$(s_t,b_t)$为输入，结构可类似Actor的MLP但输出单一标量$V_\phi(s_t,b_t)$估计状态价值。价值网络参数$\phi$通过时序差分(TD)误差训练。因为$b_t$是以$\psi$参数生成，价值损失对$\psi$也有梯度（通过$V_\phi(s,b)$对$b$的依赖），这确保信念提取不仅关注策略优化，也兼顾价值预测的准确性。
\end{itemize}

上述结构实现了一个基于隐含\textbf{belief状态}$b_t$的增强状态空间$\tilde{s}_t=(s_t,b_t)$。
从理论上讲，如果Transformer能够精确地提取出真实的环境模式（即$b_t$充分刻画$z_t$），则$\tilde{s}_t$对智能体而言就是完全可观测的，此时环境转化为\textbf{静态MDP}。
策略$\pi(a|s,b)$针对的是该等价MDP，能取得与知晓真实上下文时几乎相同的性能。
这为我们针对非平稳环境引入隐变量推断的策略提供了理论基础——智能体通过内部维护的信念将问题转化为一个随时间演化的后验估计，从而在\textbf{belief-MDP}上应用标准强化学习方法。

\medskip
\noindent (3) \textbf{训练目标与推导}\quad
LB-KLAC的训练目标在标准Actor-Critic目标上加入了\textbf{信念后验建模和正则化}项，可视为一种带约束的证据下界(ELBO)优化。
具体地，策略梯度部分遵循常规的优势函数策略优化，而信念推断部分通过KL散度项与前一步保持连续性。
综合考虑，我们在每个时间步$t$定义以下\textbf{总损失函数}并对其进行最小化：
\[
L_{\text{LB}}(\theta,\phi,\psi) \;=\; L_{\pi}(\theta,\psi)\;+\;c_V\,L_V(\phi,\psi)\;+\;\beta\,D_{\text{KL}}\!\Big(q_{\psi}(z \mid h_t)\,\Big\Vert\,q_{\psi}(z \mid h_{t-1})\Big)\;+\;c_H\,L_H(\theta,\psi)~,
\]
其中各部分含义如下：
\begin{itemize}
  \item \textbf{策略损失 $L_{\pi}(\theta,\psi)$}：采用策略梯度形式，$L_{\pi} = -\mathbb{E}_t\big[\log \pi_\theta(a_t \mid s_t,b_t)\,A_t\big]$。在实现时，我们使用$A_t$为优势估计，如采用单步TD优势$\hat{A}_t = r_t + \gamma V_\phi(s_{t+1},b_{t+1}) - V_\phi(s_t,b_t)$，或使用广义优势估计(GAE)累积多步优势。策略梯度的梯度$\nabla_\theta L_\pi = -\mathbb{E}[\nabla_\theta \log\pi_\theta A_t]$引导提高对优势为正的动作概率、降低优势为负的动作概率，从而提升性能。需要注意$\psi$参数也出现在$\pi_\theta(\cdot|s,b)$的输入$b_t$中，因此该项对$\psi$亦有梯度信号，这意味着信念提取网络将被优化以产生对获得高回报有利的$b_t$表示。
  \item \textbf{价值损失 $L_V(\phi,\psi)$}：采用均方误差形式，$L_V = \mathbb{E}_t\big[\frac{1}{2}(V_\phi(s_t,b_t) - R_t^{\text{target}})^2\big]$，其中$R_t^{\text{target}}$是价值的目标回报。对于单步任务，可取$R_t^{\text{target}} = r_t$（无折扣，一步结束）或包含下一状态值以降低方差：$R_t^{\text{target}} = r_t + \gamma V_{\phi_{\text{old}}}(s_{t+1},b_{t+1})$。我们在实现中采用后者进行bootstrap，以提升价值估计稳定性。$\phi$的梯度来自时序差分误差$(R_t^{\text{target}}-V_\phi)^2$，而$\psi$参数也通过$V_\phi(s,b)$的输入获得更新（鼓励生成能更准确预测奖励的信念$b_t$）。
  \item \textbf{信念平滑正则 $D_{\text{KL}}(q_\psi(z|h_t)\Vert q_\psi(z|h_{t-1}))$}：这是算法的核心新增项。它以KL散度衡量相邻时间步骤推断出的隐含上下文分布$q_t$和$q_{t-1}$之间的差异。该项通过系数$\beta$加权加入总损失中，其中$\beta>0$是预设的正则强度超参数。优化目标包含$\beta\,D_{\text{KL}}$项意味着：\textbf{若当前信念与上一时刻相比变化剧烈}，则损失增大，从而在梯度下降中产生一种约束效应，迫使$\psi$（以及通过$b_t$影响的$\theta,\phi$）的更新倾向于\textbf{减小信念变动}。反之，如果信念改变很小（表示策略认为环境基本延续之前的模式），该项损失接近0，则训练主要根据策略/价值损失来更新。这相当于为模型提供了一种“惯性”：除非有强烈的证据表明环境发生改变，否则不要轻易放弃先前的信念。这一机制能够有效防止智能体对单次随机波动作出过度反应（\textbf{误警}），从而提升训练的稳定性。同时，当真实环境已经改变且新证据充足时，策略依然可以通过累积的优势信号推动信念$b_t$发生较大更新，只是过程会较为“平滑”而非骤变。换言之，KL正则项提供了对策略和信念更新的\textbf{锚定}(anchoring)作用，使智能体在非平稳环境中避免灾难性遗忘：过去学到的有用策略不会被新数据迅速覆盖，一旦环境返回旧模式，智能体仅需微调信念即可恢复先前策略。
  \item \textbf{熵奖励 $L_H(\theta,\psi)$}：为避免探索不足，我们可加入策略熵的惩罚项$L_H = -\mathbb{E}_t[\mathcal{H}(\pi_\theta(\cdot|s_t,b_t))]$，即鼓励策略的熵增大。在实现中，这通常以权重$c_H$（例如0.01左右）乘以负熵加到总损失中。其效果是使策略在未充分确定最优动作时保持一定的随机性，从而更有机会探索新的动作输出组合。注意由于$\psi$影响策略输出的概率分布，该熵项同样对$\psi$有梯度，鼓励产生多样性更大的信念$b_t$。
\end{itemize}

有了上述各项，\textbf{总损失} $L_{\text{LB}}$ 综合了策略优化、价值估计、信念约束和探索需求。
我们对$\theta,\phi,\psi$求解$\min L_{\text{LB}}$，常用的方法是随机梯度下降（如Adam优化器）。
因为$\psi$贯穿于$\pi$和$V$的输入，梯度传播时$\psi$将同时受到\textbf{回报驱动}（通过$L_\pi,L_V$）和\textbf{平滑驱动}（通过$D_{\text{KL}}$）的影响。
这种联合更新实现了策略更新和上下文推断的\textbf{协同演化}：$\psi$既被奖励信号引导去辨别能够解释高奖励的上下文变化，也被正则项约束避免无根据的大幅波动。
\textbf{直观解释}：这类似于贝叶斯元学习中的后验更新，只不过将其融入策略梯度框架。
$D_{\text{KL}}(q_t\Vert q_{t-1})$惩罚过大的“belief惊异度”，使模型在没有强证据时保持对过去模式的记忆，从而当环境往复变化(例如模式A$\to$B$\to$A)时能够快速返回旧模式的策略。
理论上，若$\psi$可以逼近真实的环境后验，则在最优情况下$D_{\text{KL}}(q_t\Vert q_{t-1})$只在环境真正变化时才会增大，否则保持很小。
此时，LB-KLAC的更新等价于在\textbf{已知上下文}的MDP上进行策略迭代，其性能接近于“先知”策略。
在上下文变化不过于频繁（未超出模型表达能力）的前提下，LB-KLAC 的收敛特性与标准A2C/PPO在静态环境中的收敛类似，但在非平稳场景下能够取得远优于假设环境不变算法的表现。

需要注意的是，在实现中我们通常还结合\textbf{PPO截断技巧}保证策略更新的数值稳定。
也就是说，在策略损失$L_\pi$中，对$\log\pi_\theta(a|s,b)$项采用PPO的剪切概率比$r_t(\theta)$，并在更新过程中监控$\pi$的新旧分布KL散度，以防止单次更新跨越过大。
LB-KLAC本身鼓励小步更新（通过$\delta_t$约束，见下节），这与PPO的思想相辅相成：正则项控制信念和策略缓慢演化，而截断机制避免梯度估计偏差过大，因此可以使用稍高的学习率而保持稳定。
总之，策略梯度、KL正则和截断/熵等技术相结合，使LB-KLAC在非平稳环境中实现\textbf{稳定高效}的训练。

\medskip
\noindent (4) \textbf{可变信赖域 $\delta_t$ 机制}\quad
除了上述KL正则对信念演化进行约束外，LB-KLAC还通过一个\textbf{随时间自适应}的策略信赖域半径$\delta_t$对每次策略更新的幅度施加直接限制。
\textbf{信赖域}的思想来源于信赖域策略优化(TRPO)：要求新旧策略在每个状态上的分布差异不能超过阈值$\delta$，即
$D_{\text{KL}}(\pi_{\theta_{\text{new}}}(\cdot|s,b)\,\Vert\,\pi_{\theta_{\text{old}}}(\cdot|s,b)) \le \delta$。
这确保策略参数更新不会偏离旧策略过远，从而保证策略单调改进和稳定性。
然而，在非平稳环境中，不同时间应允许\textbf{不同大小}的策略步长：环境稳定时，应保持$\delta_t$较小以免过度调整；环境突变时，则需要临时放宽$\delta_t$以\textbf{加快适应}。
LB-KLAC通过以下两种途径实现$\delta_t$的自适应调节：

\textbf{途径一：基于启发的动态调整。}
我们利用智能体的\textbf{预测误差}或\textbf{突变指标}来判断当前环境是否发生变化，并相应放宽或收紧信赖域。
一种简单而有效的指标是\textbf{价值残差}$\varepsilon_t$，即当前奖励与Critic先验估计的差值：
$\varepsilon_t = \big|r_t - V_{\phi_{\text{old}}}(s_t, b_t)\big|$。
当$\varepsilon_t$显著偏大时，表示在状态$s_t$下实际得到的奖励与旧价值预测差异很大，暗示环境动力学或奖励规律可能发生了改变——策略对当前情境“措手不及”。
因此，我们认为出现\textbf{潜在漂移}，需要更大的策略更新步长快速追踪变化。
反之，若$\varepsilon_t$很小，说明环境与策略预测基本一致，没有明显新信息，则维持一个小步长以稳健优化。
基于此，我们设定阈值$\varepsilon_{\text{OOD}}$（Out-of-Distribution阈值）检测异常，当$\varepsilon_t$超过阈值时判定发生\textbf{漂移警报}，动态调整$\delta_t$：
\[
\delta_t \;=\;
\begin{cases}
\min(\delta_{\max},\; \kappa_{\uparrow}\,\delta_{t-1})~, & \text{若 }\varepsilon_t > \varepsilon_{\text{OOD}} \text{（出现异常差异）},\\[1ex]
\max(\delta_{\min},\; \kappa_{\downarrow}\,\delta_{t-1})~, & \text{若 }\varepsilon_t \le \varepsilon_{\text{OOD}} \text{（无显著异常）}.
\end{cases}
\]
这里$\kappa_{\uparrow}>1$是扩张系数（如1.5），$\kappa_{\downarrow}<1$是收缩系数（如0.9），$\delta_{\max}$和$\delta_{\min}$分别是信赖域半径的上下限，用于防止$\delta_t$无限放大或缩小。
通过上述规则，$\delta_t$会根据环境变化程度进行\textbf{几何型}调整：当检测到可能的环境变化时，$\delta_t$乘以$\kappa_{\uparrow}$放宽（例如从0.1增至0.15），允许接下来策略有更大的更新幅度以适应新情况；当环境平稳时，$\delta_t$逐步乘以$\kappa_{\downarrow}$收紧（如从0.15降至0.135），使策略更新更谨慎。
整个调整是平滑连续的，且只有在检测到持续异常时才会多次放宽，避免了单步噪声导致的$\delta$剧烈抖动。
此外，$\varepsilon_{\text{OOD}}$的设定应结合环境噪声水平选择，可通过监控价值残差的均值和方差来设定，例如以残差滚动平均和标准差计算一个动态阈值，保证只有在置信度较高的情况下才触发策略“大步走”。

上述基于价值残差的方法是一种不直接依赖环境真值的\textbf{无监督变化检测}：当环境发生概念漂移时，往往会表现为策略的预测误差突然增大，因此这是一个有效的启发。
在LB-KLAC实现中，我们也可以选择其他指标，如\textbf{信念KL增量}：即直接以
$D_{\text{KL}}(q_\psi(z|h_t)\Vert q_\psi(z|h_{t-1}))$
的大小作为变化指示。
如果该KL值在某一步骤骤增，意味着新接收到的观测与旧信念严重不符，也暗示环境可能变换。
这种方法无需Critic，但由于$q_\psi$本身受KL正则影响，其变化滞后性更强，实践中可以和价值残差结合使用，提高检测的鲁棒性。

\textbf{途径二：基于元学习的策略。}
除了启发式规则，$\delta_t$还可以作为一个需学习的参数，由\textbf{元梯度}自动调节。
具体而言，我们将信赖域半径的选择视为影响策略长期回报的高阶决策。
定义$J(\theta(\cdot;\delta))$为在信赖域参数$\delta$控制下经过训练得到的策略性能评价（例如整个训练过程的累积奖励或最终测试奖励），希望直接优化$\delta$以最大化$J$。
由于$\delta$影响每次策略更新的幅度，其对最终绩效的作用是通过整个训练轨迹体现的，因而$\nabla_{\delta} J$被称为元梯度（meta-gradient）。
在理论上，我们可以通过对训练过程进行微分来得到这个梯度，并采用梯度上升更新$\delta$。
实际实现中，由于对完整训练过程求导的计算和存储开销巨大，常用近似方法：例如每隔一定更新周期评估不同$\delta$对短期性能的影响，或者只对有限步的近似上升方向进行估计。
一个思路是在实现中将$\delta$设为可训练变量，将每次策略更新视为依赖$\delta$的运算，利用自动微分框架累积若干步的梯度后，对$\delta$执行一次更新。
也可以采用\textbf{多臂Bandit}或\textbf{强化学习}的方法调整$\delta$：例如在每个新环境段开始时，根据上一个段的适应表现选择更大或更小的$\delta$。
需要注意元学习$\delta$易受噪声影响，我们可以结合策略稳定性约束，确保$\delta$的学习不会本身引入过大不稳定。

无论采用何种途径，我们都会将\textbf{信赖域约束}应用于策略参数更新中。
在实现中，这通常有两种方式：(i) \textbf{显式投影}：在每次参数更新前后计算新旧策略的KL散度，若超过$\delta_t$则缩小步长，直到满足约束；
(ii) \textbf{惩罚项近似}：在损失函数中增加$\lambda\,D_{\text{KL}}(\pi_{\theta_{\text{new}}}\Vert\pi_{\theta_{\text{old}}})$的惩罚，以较大的$\lambda$迫使最优解满足近似约束。
这两者在小步长情况下等价。
我们在LB-KLAC中采用方法(i)：即预先设定$\delta_t$，尝试一次基于梯度的更新，\textbf{评估} $\KL(\pi_{\theta_{\text{new}}}\Vert\pi_{\theta_{\text{old}}})$，
如果超过$\delta_t$则按比例缩减梯度步长$\alpha$（或者进行简化的\textbf{回退}/Line Search）。
这一过程确保了每次策略更新的KL变化被控制在阈值内。
这种机制配合$\delta_t$的自适应调整，使得算法能在需要时灵活加大步长，又在平常时严格限制步长，相当于一个\textbf{安全网}，保障训练不因为瞬时异常而发散。

\medskip
\noindent (5) \textbf{变化检测与解释性输出}\quad
LB-KLAC算法不仅通过内部机制对变化做出响应，还在输出上提供对环境变化的指示，方便监控和解释智能体行为。

首先，\textbf{变化检测}可借助算法内部若干指标实现：
\begin{itemize}
  \item \textbf{信念KL散度}：$\Delta_t = D_{\text{KL}}(q_\psi(z|h_t)\Vert q_\psi(z|h_{t-1}))$直接量化了智能体对于当前环境与前一步相比的认知变化程度。如果$\Delta_t$显著升高，说明新观测让智能体显著修正了对环境的信念，很可能意味着环境发生了改变。因此$\Delta_t$可以作为变化检测的灵敏指标。在实践中，为了降低噪声带来的误报，可对$\Delta_t$做平滑处理，如取最近$k$步的平均KL或中位数，当其持续高于某阈值时再判断发生真正漂移。
  \item \textbf{价值残差/预测误差}：$\varepsilon_t = |r_t - V_{\phi_{\text{old}}}(s_t,b_t)|$同样是重要信号。当$\varepsilon_t$连续异常偏高时，意味着以当前策略评估环境出现了系统偏差。前述$\delta_t$调整已经利用该信号来触发大步更新，我们也可将其用于监控。当$\varepsilon_t$在多个时间步维持高位，可以认定环境分布已变迁。
  \item \textbf{奖励统计}：对最近窗口的平均奖励或成功率进行统计检测。例如，可采用累积和检验(CUSUM)或Page-Hinkley检测来判断奖励分布是否产生结构性突变。这些统计方法可以在较低假警率下检测出分布均值的转变，结合LB-KLAC内部信号能更加可靠地确定变化点。
  \item \textbf{策略分布变化}：虽然有$\delta_t$约束，但若环境实质变化，新策略与旧策略仍会产生一定差异。监控$\KL(\pi_{\theta_t}\Vert \pi_{\theta_{t-1}})$或策略熵等，也能补充反映策略调整的幅度。当策略对某一动作的概率发生突跳且这种改变与环境模式已知变化吻合时，说明算法正确地捕捉并应对了变化。
\end{itemize}

综合以上指标，我们可以实现一套稳健的\textbf{变化检测系统}。
工程上建议对关键指标进行日志记录和可视化：例如，将每次决策的$\Delta_t$、$\varepsilon_t$、$\pi$熵值等写入日志（如\texttt{rl\_trace.csv}的附加列），并在训练过程中实时绘制曲线，辅助研究者判断算法何时“察觉”到了环境变化。
如果需要进一步减少误警，可设置\textbf{双阈值滞后}：如要求$\Delta_t$连续$n$步超过阈值才判定变化开始，低于另一较低阈值$m$步才判定变化结束，避免边界噪声反复横跳。

其次，LB-KLAC提供了丰富的\textbf{可解释性输出}，使我们能够洞察策略决策依据：
\begin{itemize}
  \item \textbf{注意力权重}：Transformer编码器的自注意力机制使得隐含信念$b_t$是由历史序列不同部分加权得到的。通过提取Transformer最后一层对各历史输入的注意力得分，我们可以知道智能体在形成当前信念时“关注”了哪些过去事件。例如，在环境发生改变后，Transformer往往会对变化发生后的几步观测赋予最高权重，而忽略更早之前的过时信息。这与我们期望的“关注近期关键转变”相符，因此注意力模式可以作为策略依据的直观解释。我们可以将注意力权重随时间绘制成热力图，观察在每个时刻$t$，模型将注意力集中在哪几个过去时刻。当注意力焦点突然前移（从过去远处跳到最近处），往往对应环境刚发生一次变化。研究者也可检查这些被关注的历史事件的具体内容（状态、动作、奖励），理解智能体因何调整了策略。
  \item \textbf{信念嵌入/上下文ID}：虽然$b_t$本身是一个连续向量，但我们可以对其进行降维或聚类以提取更高层次的\textbf{上下文标识}。例如，采用t-SNE或PCA将所有记录的$b_t$投影到二维平面，可以观察它们是否分成几簇，每簇对应一种环境隐模式。如果环境实际模式种类有限（如A/B/C三种），我们预期$b_t$将自适应地聚类成对应的三团。在有监督情况下（仿真环境我们知道每步的真实phase标签），我们甚至可以给每个$b_t$打上真值标签，验证Transformer是否学出了正确的后验。工程实现中，可增设一个\texttt{context\_id}字段，将当前信念向量所属的簇编号记录下来。这样在日志中我们可直接看到算法判定的“当前上下文ID”，并对比真实环境模式ID以评估准确性。在应用中，即使未知真值，我们也可通过context\_id的变化频率来估计环境变化：若context\_id长时间稳定，说明算法认为处于同一模式；若频繁跳变，则可能算法过敏或环境极不稳定。
  \item \textbf{漂移评分}：结合信念变化和价值残差，我们可以定义一个\textbf{综合漂移评分}（drift score），例如$\text{score}_t = \frac{\Delta_t}{\Delta_t + 1} \cdot \min(1, \frac{\varepsilon_t}{\varepsilon_0})$（这里只是示例，可以设计成0--1范围的无量纲量）。这样的评分在环境无变化时接近0，有变化时接近1，可记录在日志并随时间绘制，从而实时指示环境漂移的程度。我们的实现中，已在环境步骤的info或trace日志中增加了\texttt{drift\_score}字段留作扩展。
  \item \textbf{价值函数残差分析}：价值网络的预测误差除了用于变化检测，也可帮助解释\textbf{性能损失}的来源。如果在某段时间内$\varepsilon_t$持续为正（实际奖励高于预测），意味着策略低估了当前环境的可获奖励，往往对应策略尚未充分开发新的最优动作；反之若$\varepsilon_t$持续为负，则策略过于乐观，可能在执行某动作时经常拿不到预期的奖励。这些信息提示我们在哪些情境下策略可能需要调整。此外，监控\textbf{优势分布}也有助解释决策：当环境变化时，旧策略在新的状态下优势分布可能偏负（许多动作都比估计的差），这是促使策略改变的直接原因。
\end{itemize}

通过以上多角度的解释性输出，我们可以\textbf{重构智能体的决策过程}：从注意力看到它参考了哪些历史事件，从信念向量或上下文ID判断其对环境所处模式的理解，从漂移评分判断环境变化强度，从价值残差了解策略预测偏差。
这样不仅有助于开发者调试和改进算法，也增强了算法的\textbf{透明度}，便于向终端用户展示“算法为何这么做”。
尤其在实际调度系统中，这种可解释性非常重要——运营人员希望了解AI决策背后的依据，以建立信任并在异常时进行人为干预。
LB-KLAC通过内置的Transformer注意力和Bayesian信念，使这一点成为可能。

\medskip
\noindent (6) \textbf{伪代码}\quad
下面给出LB-KLAC算法的训练与决策流程伪代码（Algorithm 2）。
该伪代码涵盖了从环境交互到参数更新的完整过程，包括经验缓存和信赖域控制等实现细节。

\begin{algorithm}[H]
\DontPrintSemicolon
\caption{Latent Belief KL-Regularized Actor-Critic (LB-KLAC)}
\KwInit{Initialize policy parameters $\theta$, value parameters $\phi$, inference parameters $\psi$; Initialize (optional) trust-region radius $\delta$ (e.g. $\delta=0.1$)}
\KwInit{Initialize empty replay buffer $\mathcal{D}$}
\KwParam{Hyperparameters: KL regularization weight $\beta$, initial trust-region $\delta$, trust region bounds $\delta_{\min},\delta_{\max}$, adjust factors $\kappa_{\uparrow},\kappa_{\downarrow}$, surprise threshold $\varepsilon_{\text{OOD}}$, batch size $B$, update interval $N_{\text{update}}$, learning rate $\alpha$, discount $\gamma$}
\BlankLine
\For{each incoming uncertain event (time step $t$)}{
  Observe current state $s_t$ \;
  Obtain history $h_t$ (sequence of last $H$ transitions up to $s_t$) \;
  Compute belief embedding $b_t = f_\psi(h_t)$ \tcp*{Transformer inference of context}
  Select action $a_t \sim \pi_\theta(\cdot \mid s_t,\,b_t)$ \tcp*{Sample from current policy (could use $\epsilon$-greedy or softmax exploration)}
  Execute action $a_t$ in environment \;
  Observe reward $r_t$ and next state $s_{t+1}$ \;
  Store transition $(s_t,\; b_t,\; a_t,\; r_t,\; s_{t+1})$ into buffer $\mathcal{D}$ \;
  \BlankLine
  \tcp*[l]{Optional: online change detection and trust-region adaptation}
  Compute prediction error $e_t = \big| r_t - V_{\phi}(s_t, b_t) \big|$ \;
  \uIf{$e_t > \varepsilon_{\text{OOD}}$ {\bf or} $D_{\text{KL}}\!\big(q_\psi(z|h_t)\,\Vert\,q_\psi(z|h_{t-1})\big) >$ high threshold}{
      $\delta \leftarrow \min(\delta_{\max},\; \kappa_{\uparrow}\,\delta)$  \tcp*{loosen trust-region radius}
  }
  \Else{
      $\delta \leftarrow \max(\delta_{\min},\; \kappa_{\downarrow}\,\delta)$  \tcp*{tighten trust-region radius}
  }
  \BlankLine
  \tcp*[l]{Periodic policy/value update}
  \If(\tcp*[f]{update every $N_{\text{update}}$ transitions}){$t$ mod $N_{\text{update}} == 0$}{
    Sample mini-batch $\mathcal{B}$ of transitions from $\mathcal{D}$ (prioritize most recent) \;
    \ForEach{transition $(s_i, b_i, a_i, r_i, s_i')$ in $\mathcal{B}$}{
       Compute $b_i' = f_\psi(h_{i+1})$ for next state $s_i'$ (using updated history) \;
       Compute advantage $\displaystyle \hat{A}_i = r_i + \gamma\,V_{\phi}(s_i', b_i') - V_{\phi}(s_i, b_i)$ \;
       $L_{\pi} \mathrel{+}= -\log \pi_{\theta}(a_i \mid s_i, b_i)\;\hat{A}_i$ \tcp*{policy gradient loss (PPO clipping can be applied here)}
       $L_{V} \mathrel{+}= \frac{1}{2}\Big(V_{\phi}(s_i, b_i) - \big(r_i + \gamma\,V_{\phi}(s_i', b_i')\big)\Big)^2$ \tcp*{value TD loss}
       $L_{\text{KL}} \mathrel{+}= D_{\text{KL}}\!\Big(q_\psi(z \mid h_i)\,\Vert\,q_\psi(z \mid h_{i-1})\Big)$ \tcp*{belief smoothness loss}
       $L_{H} \mathrel{+}= -\mathcal{H}\!\big(\pi_\theta(\cdot \mid s_i, b_i)\big)$ \tcp*{entropy bonus (optional)}
    }
    Normalize losses: $L_{\pi} \mathrel{/=} |\mathcal{B}|$, $L_{V} \mathrel{/=} |\mathcal{B}|$, $L_{\text{KL}} \mathrel{/=} |\mathcal{B}|$ (and $L_H$ similarly) \;
    Compute total loss $L_{\text{total}} = L_{\pi} + c_V L_{V} + \beta\,L_{\text{KL}} + c_H L_{H}$ \;
    Compute gradients $(\nabla_\theta,\nabla_\phi,\nabla_\psi) = \nabla_{\theta,\phi,\psi} L_{\text{total}}$ \;
    Propose parameter update: $(\theta',\phi',\psi') = (\theta,\phi,\psi) - \alpha\,(\nabla_\theta,\nabla_\phi,\nabla_\psi)$ \;
    Evaluate policy divergence $\displaystyle \Delta_{\pi} = D_{\text{KL}}\!\Big(\pi_{\theta'}(\cdot \mid s, b)\;\Big\Vert\;\pi_{\theta}(\cdot \mid s, b)\Big)$ averaged over batch states \;
    \If(\tcp*[f]{ensure policy update within trust region}){$\Delta_{\pi} > \delta$}{
       Scale gradient: $(\nabla_\theta,\nabla_\phi,\nabla_\psi) \leftarrow \frac{\delta}{\Delta_{\pi}}\,(\nabla_\theta,\nabla_\phi,\nabla_\psi)$ \tcp*{reduce step size proportionally}
       Update $(\theta',\phi',\psi') = (\theta,\phi,\psi) - \alpha\,(\nabla_\theta,\nabla_\phi,\nabla_\psi)$ \;
    }
    Commit parameter update: $(\theta,\phi,\psi) \leftarrow (\theta',\phi',\psi')$ \;
  }
}
\end{algorithm}

\medskip
\noindent \textbf{（伪代码之后的实现补充说明）}\quad
上述算法流程中，我们每收到一个环境事件（状态）就依据当前策略$\pi_\theta$选动作并得到奖励，然后暂存经验，并定期对策略和价值网络进行一次批量更新。
经验缓存$\mathcal{D}$采用滑动窗口机制：只保留最近$N_{\max}$条经验（例如1000条），防止过陈旧的数据干扰训练。
采样小批量时对最新样本给予优先，这有效类似为近期经验赋予更高权重，使策略更快适应当前环境段。
每次更新我们对 batch 中每条经验计算一遍 Transformer 得到$b_i$（在实现中可将 Transformer 和后续网络合并以减少重复计算），然后累加计算损失。
实际实现时可对$\pi$的损失使用 PPO 剪切，以避免单些优势值异常导致策略剧烈更新；也可加入熵项$L_H$鼓励探索（伪代码中已示意）。
在应用信赖域约束时，这里采用的是平均 KL 约束近似：我们估计新旧策略在 batch 状态上的平均 KL $\Delta_\pi$，并与$\delta$比较。
如果$\Delta_\pi$超过$\delta$，则缩小梯度步长（或者退回重新试更小更新），确保更新后策略未走出信赖域。
这一近似在 batch 较大且状态分布接近真实分布时效果良好，也简化了实现。

关键超参数方面，KL 正则权重$\beta$控制信念更新速度，一般取$0.1 \sim 1.0$；策略学习率$\alpha$可适当偏高（如$5\times10^{-4}$），因 KL 项提供了稳定剂作用；信赖域$\delta$初值可设$0.05\sim0.2$（对应策略 KL 的典型初始步长），并设置$\delta_{\min}$和$\delta_{\max}$保证$\delta_t$在合理范围（如$[0.01, 0.5]$）；$\kappa_{\uparrow},\kappa_{\downarrow}$可取$1.5/0.9$或$2.0/0.8$等，具体取决于希望对变化的敏感程度。
Transformer 结构与算法1使用的 Transformer 类似，但可以适当增大容量以表征更复杂的上下文后验，例如增加隐藏维度或输出多模态参数。
隐含上下文 embedding 维度$|b|$也是一个超参：太小可能无法分辨复杂模式，太大会导致过拟合；一般在10到50之间，根据模式类别和历史长度调整。
经验批量大小$B$和更新间隔$N_{\text{update}}$决定了每收集多少样本更新一次；由于本问题样本获取代价高，我们倾向于小$B$高频更新（例如每4--8步更新一次，每次$B=32$左右），以充分利用新数据。
为防止旧环境段数据的干扰，我们也可以在经验采样时执行片段隔离：例如检测到环境已切换一个新模式后，逐渐减少对前一模式数据的采样概率，或者直接在缓冲中标记不同段的数据做分段采样。
实践中，我们发现 LB-KLAC 只需关注最近数据，Transformer 本身会“淡忘”旧数据模式，同时 KL 项对长时间跨度的信念差异会自然趋于较大值使旧段数据损失变高，等效地降低了旧数据的权重。

最后，计算复杂度方面，LB-KLAC 的主要开销在 Transformer 的前向和反向传播，单次复杂度约$O(H \cdot d \cdot |\mathcal{B}|)$（$H$为历史序列长，$d$为注意力头数或隐层维度）。
例如$H=50,d=4,|\mathcal{B}|=32$时每次更新操作在现代 GPU 上仅需几十毫秒量级，可接受。
存储开销主要来自经验缓冲和 Transformer 参数，但在状态维度很低（2--3维）且$|b|$适中（几十维）的情况下，这些参数总量可能仅数万，远小于典型深度网络。
训练时可充分利用 GPU 的并行能力，将小批量样本的 Transformer 推理打包在一起计算，并利用自动微分轻松获得梯度。
整体而言，LB-KLAC 能够以较小的计算代价运行在双线程调度架构中，实现非平稳环境下的高效学习。

\medskip
\noindent (7) \textbf{工程实现建议}\quad
将 LB-KLAC 融入现有的 ALNS+RL 双线程架构，需要在软件接口、并发控制和算法模块上做一些改动和优化。

\textbf{双线程架构对接：}
根据用户提供的框架，RL 以 Gym 环境形式在独立线程运行，与 ALNS 线程通过共享的 state\_reward\_pairs 表通信。
环境每次 \texttt{reset()} 阻塞等待 ALNS 插入一条带占位动作的事件，然后提供状态给 RL；RL 决策后调用 \texttt{step(action)} 写入动作并等待 ALNS 计算出奖励写回。
由于 episode 长度被设置为 1，传统 RL 库会将每个事件视作独立样本，不保留跨事件的状态。
这对 LB-KLAC 这样的算法是不可取的，因为我们需要跨事件维护历史$h_t$以推断上下文。
因此，工程上需要绕过 Gym 的自动 reset 行为，在 RL 线程中自行维护历史。
具体而言，可在 coordinationEnv 之外实现一个全局的历史队列：当环境 \texttt{reset()} 提供初始状态$s_t$时，不立即清空历史，而是检查上一步是否 episode 结束（通常会结束，因为 episode\_length=1 设置），但我们将之前 episode 的$(s_{t-1},a_{t-1},r_{t-1})$保留用于新的历史序列。
这相当于将多个单步 episode 串联看作一个连续时间序列，只是在内部逻辑上 episode 边界用于日志分段。
更直接的做法，是不使用 Gym 内部的 VecEnv rollout 机制，而是在 RL 线程手动控制环境循环（参见下文 Agent 接口），这样可以完全按我们的需求拼接历史。

\textbf{自定义训练循环 vs.\ SB3 改造：}
由于 LB-KLAC 需要定制 Transformer 网络和损失计算，直接使用 Stable Baselines3 (SB3) 的现有算法（PPO/A2C 等）将很困难。
SB3 虽提供自定义 policy 网络的接口，但我们还需将 KL 正则项纳入损失，并实现自适应$\delta_t$约束，这超出了简单 callback 能处理的范围。
因此，推荐编写自定义训练循环，类似于前述伪代码。
这可以参考用户框架中 DRCB 算法的接入方式：在 \texttt{Dynamic\_master} 中增加算法选项，通过条件选择在 RL 线程运行我们新实现的 LB-KLAC 算法类，而不调用 SB3 的 \texttt{model.learn()}。
为此，可在 \texttt{codes/robust\_rl/} 目录中新建 \texttt{lbklac.py} 模块，定义 \texttt{LBKLACAgent} 类，实现以下接口：

\begin{verbatim}
class LBKLACAgent:
    def act(self, obs, *, deterministic=False) -> int: ...
    def observe(self, obs, action, reward, next_obs, done) -> None: ...
    def update(self) -> None: ...
    def save(self, path) -> None: ...
    @classmethod
    def load(cls, path, **kwargs) -> "LBKLACAgent": ...
\end{verbatim}

该类封装 LB-KLAC 算法的策略网络和训练逻辑。
其中 \texttt{act()} 根据当前策略输出动作（这里会包含计算$b_t$的过程），\texttt{observe()} 接收环境返回的结果并存储经验，\texttt{update()} 执行一次策略参数更新（可以按照伪代码来实现）。
这样在 RL 线程的主循环中，就可以写成：

\begin{verbatim}
agent = LBKLACAgent(**hyperparams)
obs = env.reset()
while not training_done:
    action = agent.act(obs)
    next_obs, reward, done, info = env.step(action)
    agent.observe(obs, action, reward, next_obs, done)
    if should_update:
        agent.update()
    if done:
        obs = env.reset()
    else:
        obs = next_obs
\end{verbatim}

上面逻辑与传统 Gym 训练类似，但我们掌控了何时清空历史、何时训练更新等细节。
注意，当 episode\_length=1 时，每步后环境都会返回 \texttt{done=True}，因此我们需要跨 episode 保存历史：可在 \texttt{observe()} 中检测到 done 但如果还未到仿真整体结束，就将 \texttt{next\_obs} 当作新 episode 起点继续记录于历史序列。
或者更简单地，我们让环境的 done 始终为 False（除了仿真整体结束时）来骗过 Gym，这样 RL 线程会认为是单个持续不断的 episode。
这需要修改 \texttt{coordinationEnv.step()} 返回的 done 字段，但考虑评估代码依赖 episode 划分，最好还是在 Agent 内部处理，将历史缓存和 episode 逻辑解耦。

\textbf{Transformer 模块优化：}
Transformer 的计算较昂贵，因此要充分优化。
首先将$\psi$的实现复用：无论 Actor 还是 Critic，都调用同一份 Transformer 得到$b_t$，在一次时间步决策中只前向计算一次 Transformer。
我们可以将 Transformer 和后续 Actor 头部整合成一个整体 PyTorch 模型，从而一次前向既输出动作概率也输出价值。
其次，使用并行批量计算提高效率：例如在 \texttt{update()} 中，一次性将 batch 所有$h_t$通过 Transformer 处理，可以利用矩阵乘法的并行性。
我们的实现可预先将历史序列转换为张量（形状$|\mathcal{B}| \times H \times d_{\text{in}}$），在一个前向中得到所有$b$。
另外可使用 GPU 加速：将模型和数据张量搬到 CUDA 设备进行前向/后向计算，尤其在并行计算注意力时 GPU 的速度优势明显。
如果机器不支持 GPU，则可考虑将 Transformer 层数降到1、隐藏维度减小等方式，以确保实时性。

\textbf{与现有代码的集成修改：}
按照上述设计，所需改动的现有模块包括：
\begin{itemize}
  \item 在 \texttt{Dynamic\_master34959.py} 中，增加对 \texttt{--algorithm LBKLAC} 参数的解析，并在启动 RL 线程时传递算法选择（类似已有的 DRCB 分支）。
  \item 在 \texttt{dynamic\_RL34959.py} 中，在初始化模型处增加 \texttt{if algorithm == "LBKLAC": model = robust\_rl.lbklac.LBKLACAgent(...)}，并调整训练循环：对于 LBKLAC，不调用 \texttt{model.learn()} 而是使用前述自定义循环，不断地与环境交互并调用 \texttt{agent.update()}。
        这一改动会涉及绕过 SB3 的 VecEnv，将我们的 Agent 接口函数挂接进去，同时保持原有日志记录函数调用（如 \texttt{log\_training\_row} 记录每步训练信息）。
  \item 在 \texttt{Intermodal\_ALNS34959.py} 中，检查并保持与 RL 侧日志字段一致。根据提供的 MD，已经增加了 \texttt{algo, regime\_id, context\_id, drift\_score} 等字段；我们需要确保 LBKLAC 在决策时能填充相应字段（例如将 \texttt{context\_id} 设为当前 belief 向量聚类 ID，或先留空用于后续离线分析；将 \texttt{drift\_score} 设为当前$\Delta_t$或$\varepsilon_t$等）。
        可以考虑在 \texttt{dynamic\_RL34959.py} 的 \texttt{log\_trace\_from\_row} 调用前，读取 Agent 中最近计算的$\Delta_t$和$b_t$，附加到 trace 日志，以便在 \texttt{compute\_metrics.py} 或 \texttt{plot\_paper\_figure.py} 中使用。
  \item 增加 \texttt{codes/robust\_rl/lbklac.py} 文件：实现 \texttt{LBKLACAgent} 类以及 Transformer+Actor+Critic 的模型。
        可以参照 PyTorch 范式，定义 \texttt{forward(history)} 输出$\pi$和$V$，内部包含 \texttt{TransformerEncoder} 层和 MLP 层。
        注意历史可以先 embed 再加位置编码后送入 Transformer。
        信赖域约束的 KL 计算可通过在 Agent 内部保存旧策略参数副本或旧策略网络$\pi_{\text{old}}$实现：每次 \texttt{update} 前拷贝一次$\theta_{\text{old}}$，update 后用新旧网络计算 KL。
        如果采用近似方法如在 update 中估计$\Delta_\pi$，可利用小批量的$\pi$输出近似。
  \item （可选）修改 \texttt{run\_experiments\_server.py} 或相关实验脚本，将 LBKLAC 纳入可选算法列表，使得批量实验时能方便地启动 LBKLAC 训练。
\end{itemize}

\textbf{与 ALNS 信息交换：}
基于 MD 描述，RL 需要从 Excel 元数据中读取一些环境漂移信息用于日志（如真值 phase 标签）。
目前实现是在 \texttt{get\_state()} 时读出了 \texttt{current\_phase\_label} 存入全局变量，再在 trace 中记录。
LB-KLAC 算法本身不直接使用这些真值，但为了评估，可继续记录。
建议保持现有机制，将 \texttt{phase\_label} 通过 trace 日志保存，以便对比算法自己的推断。
另一方面，我们希望算法本身不依赖任何真值信号，以证明其完全通过历史推断上下文。
因此，在 Env.\texttt{step()} 返回的 \texttt{info} 中不注入任何监督信号（已保证 obs 中不含 phase\_label），只在日志层面使用这些数据。

\textbf{训练稳定性与样本效率考虑：}
ALNS 仿真步骤非常昂贵（每个事件可能涉及复杂的优化计算），因此我们必须最大化每条样本的价值。
为此，一是使用经验重放和小批多次更新：LB-KLAC 在每收集一次交互后，可以对新数据重复训练$K$轮（比如$K=1$--$5$），这在我们的伪代码中通过 batch 采样近期数据并多步梯度下降体现。
二是并行收集：如果可能，同时运行多个独立的环境实例（例如多个不同随机种子或不同 pattern 的 Excel 表）来收集数据，然后共享参数。
不过本项目主要依靠多次独立 run 并行，而单 run 内由于 ALNS 线程已占用一个核，开启并行环境意义不大。
因此我们采取顺序训练但重复使用数据的策略，确保每个样本贡献尽可能多的梯度信息。
实验证明，在我们的设置下，LB-KLAC 每获取数百样本即可收敛，相比无适应机制的算法需要上万样本才能在变化后恢复性能，效率提升明显。

\medskip
\noindent (8) \textbf{实验与消融设计}\quad
为验证 LB-KLAC 在非平稳环境下的有效性，我们设计了一系列实验，包括与基线的对比和对算法内部组件的消融分析。

\textbf{对比基线：}
首先，将 LB-KLAC 与多种现有方法进行性能比较，以体现其优势：
\begin{itemize}
  \item \textbf{Naive RL 算法：} 如 PPO、A2C、DQN 等不含任何非平稳处理的传统算法。它们在环境变化时无法快速适应，我们预期其性能（平均奖励）在变化点会出现明显下降且恢复缓慢。这个对比展示非平稳性对常规算法的冲击，以及 LB-KLAC 在同样条件下能保持较高表现的能力。
  \item \textbf{非平稳 RL 基线：} 选取文献中的方法作为对比，例如 DRCB 算法（“Dynamic Regime Contextual Bandit”）。DRCB 假设可以识别环境的阶段，并对不同阶段分别训练模型（或采用指数遗忘因子），属于一种简化的元学习方法。另一类基线是滑动窗口 DQN/PPO，即仅用最近固定窗口的数据训练，当作应对概念漂移的朴素方法。我们通过这些对比，检验 LB-KLAC 是否优于简单的近期加权策略。
  \item \textbf{Meta-RL 基线：} 如逐步元策略方法（例如每次变化后重置 RNN 隐状态或策略参数，类似于 Contextual Meta-RL）。这些方法通过在训练中模拟环境变化来学习快速适应，本质上相当于 RL$^2$（策略的策略）。然而在我们的在线场景，由于探索代价高，这类方法未必实用。实验将显示 LB-KLAC 在样本效率和稳定性上可能更胜一筹。
  \item \textbf{理论上限：} 引入“先知”策略作为参照，即假设智能体每次变化点立即知道新的环境模式并切换到对应的最优策略。虽无法实现，但通过计算先知策略的累积奖励，我们可以得到性能上界，用以评估 LB-KLAC 达到上界的相对程度。理想情况下，LB-KLAC 的表现应接近先知策略，尤其在变化发生后较短时间内就逼近新的最优。
\end{itemize}

\textbf{环境设置：}
我们将上述算法在多种非平稳情境下进行测试，包括：周期两种模式切换（A--B 交替）、三种模式循环（A--B--C）、随机模式（随机在若干模式中跳变）、以及带回归效应的情景（如逐渐变化或带噪声的小幅抖动）。
数据由调度仿真器生成，对应 \texttt{distribution\_config.json} 中的 \texttt{ab/aba/abc/recall/random\_mix} 等。
每个情景都跑多次随机种子取平均，以保证结果可靠。
评估以\textbf{实现阶段（implement/test）}的表现为主，即算法经过训练收敛后在后续决策中的表现。
我们记录各方法在变化过程中的即时奖励、平均奖励随时间曲线，以及累积奖励或阶段性成功率等。

\textbf{消融实验：}
为了量化 LB-KLAC 各组成部分的作用，我们进行至少4组消融对比，每次移除或修改一个组件，观察性能变化：
\begin{itemize}
  \item \textbf{无信念 KL 正则：} 将$\beta=0$关闭 KL 项，只使用 Transformer 提供的隐状态参与策略更新。预期：无 KL 约束时在噪声下易产生误警，策略波动性增大；当环境返回旧模式时更易遗忘。
  \item \textbf{固定信赖域$\delta$：} 禁用$\delta_t$动态调整，改为恒定信赖域。对比小$\delta$（保守、适应慢）与大$\delta$（激进、可能震荡/发散），以验证自适应$\delta_t$如何兼顾速度与稳定性。
  \item \textbf{无注意力的隐状态：} 将 Transformer 替换为简单历史聚合（平均池化或 RNN 最后一步等），测试注意力对快速感知突变的贡献。
  \item \textbf{信念维度与 Transformer 容量：} 减小$|b|$或减少 Transformer 层数，观察表达能力不足导致的模式混淆与性能退化；当层数降至0（无历史，仅当前状态）时，算法将退化为普通 Actor-Critic，对非平稳性几乎无抵抗力。
\end{itemize}
此外还可做超参数扫描（$\beta$、$\kappa_{\uparrow},\kappa_{\downarrow}$等）并分析稳定性与敏感度权衡。

\textbf{评估指标：}
\begin{itemize}
  \item \textbf{平均奖励：} 测试阶段每$N$步的平均奖励（或成功率）曲线；重点关注变化点处最低 reward 与恢复时间（例如恢复到变前90\%水平所需事件数）。
  \item \textbf{累积优势指标：} 相对固定策略（Always-0/Always-1）以及随机策略的归一化优势，例如
  $Adv0 = (R_{\text{RL}} - R_{\text{always0}})/n_{\text{events}}$，
  $Adv1 = (R_{\text{RL}} - R_{\text{always1}})/n_{\text{events}}$，并构造综合指标$G$用于横向比较。
  \item \textbf{策略稳定性指标：} 如策略分布 KL $\Delta_{\pi}$ 的峰值、价值损失峰值等，反映是否出现策略崩溃或价值发散。
  \item \textbf{解释性指标：} 从注意力权重、信念动态中提取如注意力重心变化$\Delta_{\text{attn}}$、信念 KL 上升同步性等，辅助定性解释工作机理。
\end{itemize}

\medskip
\noindent (9) \textbf{失败模式与对策}\quad
尽管 LB-KLAC 在理论和实验上展现出优异性能，我们仍需关注在实际应用中可能出现的失败模式，并提出相应的缓解策略：

\textbf{失败模式1：信念崩溃（Belief Collapse）。}
指信念网络$f_\psi$输出丧失区分度，可能表现为始终输出近似相同的$b_t$（对历史不敏感），或在剧烈变化后信念迟迟未能收敛到有效模式。
原因可能包括：过强正则（$\beta$过大导致信念僵硬）、模型容量不足、探索不足、误导性噪声数据等。
\textbf{缓解策略：}
调节$\beta$到适中范围并监控$b_t$多样性；提升 Transformer 容量或增大$|b|$；引入信念熵正则（例如增加$-\eta \cdot H[q_\psi(\cdot|h_t)]$项）；必要时对$\psi$做轻量“重启/校准”（如注入小噪声或重置部分层）以摆脱坏局部极小。

\textbf{失败模式2：虚警与过度适应（False Alarm \& Overreaction）。}
噪声环境下，随机波动可能被误判为模式切换，引发不必要的大步更新并降低性能。
\textbf{缓解策略：}
对$\Delta_t,\varepsilon_t$等信号做平滑与滞后；引入统计检验（CUSUM、Page-Hinkley 等）作为二次确认；在噪声大时适当降低$\alpha$和$\delta_{\max}$；对奖励/状态做简单滤波以减少短暂尖峰影响。

\textbf{失败模式3：适应不足/滞后（Slow Adaptation or Lagging）。}
变化过快或幅度过大时，策略可能跟不上；或$\delta$过小导致更新过于保守，过渡期损失较大。
\textbf{缓解策略：}
增大模型容量或减小历史窗口$H$以提高对近期趋势敏感度；放宽约束（适当提高$\delta_{\max}$等）并监控 KL 防止爆炸；（工程可选）引入专家集成或更快的元参数调整；（若系统允许）在外部已知变化时临时提高$\delta_t$并降低$\beta$以快速重塑策略。

\textbf{失败模式4：探索过低（Insufficient Exploration）。}
策略过度贪心导致另一动作长期不被尝试，环境切换后难以及时发现新最优动作。
\textbf{缓解策略：}
使用熵激励$c_H L_H$保持非零随机性；在动作选择上叠加$\epsilon$-greedy（例如$\epsilon=0.05$）；利用信念不确定性驱动探索（随信念熵增大而增大$\epsilon$或$c_H$）；回放采样中保证一定比例的探索数据，避免完全遗忘某动作的价值。
实践上可在训练初期与高不确定阶段保持小但非零探索率，并随信念确定度退火，以兼顾稳态段性能与变换段发现速度。



\end{document}
