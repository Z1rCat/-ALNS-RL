\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{float}
\usepackage{ctex}
\usepackage{algorithm}
\usepackage{algpseudocode}

\setlength{\parskip}{0.6em}
\setlength{\parindent}{0pt}

\title{基于历史注意力的非平稳单步决策改进：PPO/A2C 的 HAT--SNA 方法与工程实现}
\author{项目：ALNS+RL 双线程系统（34959）}
\date{\today}

\begin{document}
\maketitle

\section{汇报目的与结论}
本文件用于向老师汇报：我们在现有 ALNS+RL 双线程系统中，\textbf{在不改动 SB3(PPO/A2C) 核心优化器的前提下}，通过\textbf{历史包装器(History) + 注意力特征提取(Attention)}增强策略对非平稳分布突变的适应性，并给出实现逻辑、公式与复现实验信息。

\textbf{命名：}HAT = History-Attention Transform；SNA = Simple Nonstationarity Adaptation（漂移感知调度 + 推理期门控；红阶段严格无梯度更新）。

\textbf{最关键改动点：}将原始低维观测 $o_t$ 扩展为长度为 $H$ 的历史序列 $X_t$，并用轻量 Transformer 编码为上下文特征 $f_t$，再交给 SB3 的 actor/critic 网络；同时在\textbf{单步 episode} 场景下通过 \textbf{keep\_history} 机制让历史跨 reset 保留，使模型在 episode\_length=1 时仍能形成有效的 $h_t$。

\textbf{一个直接证据：}以同一场景（R30, S5, seed=42）为例，A2C 基线在测试阶段 200 次决策的平均回报约为 0.505，而 A2C\_HAT\_SNA 达到约 0.805（详见 \S\ref{sec:results}）。

\section{问题设置：单步决策 + 隐含非平稳模式}
\subsection{系统交互（ALNS+RL 双线程）}
系统由 ALNS 线程生成决策点状态并阻塞式等待 RL 决策；RL 线程输出动作后由 ALNS 计算并回填奖励。每个决策为单步 episode：
\begin{itemize}
  \item 动作空间：$a_t \in \{0,1\}$（例如 \texttt{wait/keep} vs \texttt{reroute}）。
  \item 奖励：$r_t \in \{0,1\}$（二值成功/失败反馈）。
  \item 观测：$o_t \in \mathbb{R}^d$（低维，包含如 \texttt{delay\_tolerance}、\texttt{severity} 等；严禁使用显式 regime 标签作为输入）。
\end{itemize}

\subsection{非平稳性：隐藏模式分段稳定并突变}
环境存在隐藏模式 $z_t$（分段稳定、突变），RL 只能通过历史 $h_t$ 推断：
\begin{align}
z_t &\sim \text{piecewise-constant process}, \\
o_t &\sim p(o \mid z_t), \qquad r_t \sim p(r \mid o_t, a_t, z_t).
\end{align}
由于 episode\_length=1，若每次 reset 都清空历史，则策略退化为纯 contextual bandit：$\pi(a\mid o_t)$ 无法利用跨时信息识别 $z_t$，从而对突变适应极慢。

\section{基线：SB3 的 PPO / A2C（保持不改）}
\subsection{Actor--Critic 基本形式}
我们复用 stable-baselines3 的 PPO/A2C 实现。对任意时刻 $t$，策略与价值函数为：
\begin{align}
\pi_\theta(a_t \mid s_t) \quad\text{与}\quad V_\phi(s_t),
\end{align}
其中 $s_t$ 为输入特征（基线为 $s_t=o_t$，本文改进为 $s_t=f_t$）。

\subsection{单步 episode 下的优势}
在单步 episode（done 每步为真）条件下，回报目标自然为：
\begin{align}
G_t = r_t, \qquad A_t \approx r_t - V_\phi(s_t).
\end{align}
SB3 内部仍使用通用的 GAE/TD 形式，但由于 done 信号每步截断，实际训练信号接近上述单步优势。

\subsection{A2C 的损失（SB3 原生，本文不修改）}
以单步优势为例，A2C 的典型目标可写为最小化：
\begin{align}
\mathcal{L}_{\text{A2C}}(\theta,\phi)
&=
\underbrace{-\mathbb{E}_t\bigl[\log \pi_\theta(a_t \mid s_t)\,A_t\bigr]}_{\text{policy loss}}
+ c_V \underbrace{\mathbb{E}_t\bigl[(V_\phi(s_t)-G_t)^2\bigr]}_{\text{value loss}}
- c_H \underbrace{\mathbb{E}_t\bigl[\mathcal{H}(\pi_\theta(\cdot\mid s_t))\bigr]}_{\text{entropy bonus}}.
\end{align}
其中 $\mathcal{H}(\cdot)$ 为熵，$c_V,c_H$ 为系数。本文的 HAT 只改变 $s_t$ 的构造方式（从 $o_t$ 变为 $f_t$），不改 A2C 的优化目标与更新流程。

\subsection{PPO 的 clipped 目标（SB3 原生，本文不修改）}
PPO 的核心为 clipped surrogate objective。令重要性比率
\begin{align}
\rho_t(\theta)=\frac{\pi_\theta(a_t\mid s_t)}{\pi_{\theta_{\text{old}}}(a_t\mid s_t)},
\end{align}
则其策略目标可写为：
\begin{align}
\mathcal{L}^{\text{clip}}_{\text{PPO}}(\theta)
=
\mathbb{E}_t\left[
\min\left(
\rho_t(\theta)A_t,\;
\text{clip}(\rho_t(\theta),1-\epsilon,1+\epsilon)A_t
\right)
\right],
\end{align}
并配合 value loss 与 entropy bonus 组成总目标。本文的 HAT 同样只改变 $s_t$（输入特征），不改 PPO 的核心裁剪机制。

\section{HAT：History + Attention 的模型层改进}
\label{sec:hat}
\subsection{动机：让策略看到 $h_t$ 而非只看 $o_t$}
我们将任务视为 POMDP：单步 episode 仅是执行机制，真实信息在跨步历史中。因此我们构造固定窗口历史：
\begin{align}
h_t = \bigl(o_{t-H+1}, a_{t-H}, r_{t-H}, \ldots, o_t \bigr),
\end{align}
并学习一个上下文编码器 $f_t = g_\psi(h_t)$，让 actor/critic 都在 $f_t$ 上工作。

\subsection{HistoryAttentionWrapper：历史序列化（无未来泄漏）}
我们定义动作 one-hot 编码：
\begin{align}
e(a) \in \{0,1\}^2,\quad e(a)[a]=1.
\end{align}
构造 token（只使用\textbf{过去}的动作与回报，不使用未来信息）：
\begin{align}
x_t = \bigl[\,o_t;\; e(a_{t-1});\; r_{t-1}\,\bigr] \in \mathbb{R}^{d+2+1}.
\end{align}
将最近 $H$ 个 token 堆叠得到输入序列（不足 $H$ 时零填充）：
\begin{align}
X_t = [x_{t-H+1}, \ldots, x_t] \in \mathbb{R}^{H\times (d+3)}.
\end{align}
\textbf{数据泄漏约束：}任何显式的分布真值标签（例如 phase\_label/regime\_id 等）\textbf{仅用于日志与评估}，不进入 $o_t$/$x_t$/$X_t$。
\textbf{关键工程点：}由于 episode\_length=1，环境会频繁调用 reset。为避免历史被清空，我们引入 \texttt{keep\_history} 开关：当其为真时，reset 不清空历史队列，从而跨 episode 维护 $X_t$。

\begin{algorithm}[H]
\caption{HistoryAttentionWrapper（核心逻辑）}
\begin{algorithmic}[1]
\State 初始化：队列 $\mathcal{D}\leftarrow [\;]$（最大长度 $H$），$\texttt{last\_a}\leftarrow \mathbf{0}$，$\texttt{last\_r}\leftarrow 0$
\Function{Reset}{}
\State $o \leftarrow \texttt{env.reset()}$
\If{\texttt{keep\_history} 为假 或 $\mathcal{D}$ 为空}
\State 清空 $\mathcal{D}$，重置 $\texttt{last\_a},\texttt{last\_r}$
\EndIf
\State 追加 $x=[o;\texttt{last\_a};\texttt{last\_r}]$ 到 $\mathcal{D}$
\State \Return $\texttt{stack\_and\_pad}(\mathcal{D}) \in \mathbb{R}^{H\times(d+3)}$
\EndFunction
\Function{Step}{$a$}
\State $(o', r, done, info) \leftarrow \texttt{env.step}(a)$
\State $\texttt{last\_a}\leftarrow e(a)$，$\texttt{last\_r}\leftarrow r$
\State 追加 $x'=[o';\texttt{last\_a};\texttt{last\_r}]$ 到 $\mathcal{D}$
\State \Return $(\texttt{stack\_and\_pad}(\mathcal{D}), r, done, info)$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsection{AttentionExtractor：轻量 Transformer 编码历史}
给定 $X_t \in \mathbb{R}^{H\times D}$（$D=d+3$），先线性嵌入并加位置编码：
\begin{align}
u_i &= W_e x_i + b_e,\quad p_i \in \mathbb{R}^{m}, \\
v_i &= u_i + p_i,\qquad i=1,\ldots,H.
\end{align}
对每一层 Transformer self-attention（以单头为例）：
\begin{align}
Q &= V W_Q,\quad K = V W_K,\quad V' = V W_V,\\
\mathrm{Attn}(Q,K,V') &= \mathrm{softmax}\left(\frac{QK^\top}{\sqrt{m}}\right)V'.
\end{align}
多头注意力与前馈网络堆叠 $L$ 层后，取最后一个 token 的表示作为 pooled 上下文：
\begin{align}
c_t &= \mathrm{TransformerEncoder}(X_t)_{H},\\
f_t &= W_p c_t + b_p \in \mathbb{R}^{F}.
\end{align}
其中 $F$ 为输出特征维度（我们实现中默认 $F=64$），作为 SB3 policy 的特征输入。

\textbf{要点：}attention 的作用不是追求“复杂”，而是提供一种\textbf{可学习的历史聚合}：模型可以在非平稳突变后，依靠历史 token 的统计变化来重构隐模式 $z_t$ 的“证据”，从而改变策略输出。

\section{SNA：非平稳增强（训练期调参 + 推理期门控）}
\label{sec:sna}
\subsection{漂移统计（EMA）}
为了在不改变主优化器的情况下增强非平稳适应，我们引入运行态统计量（不是网络参数）：
\begin{align}
\bar r_t &= (1-\alpha)\bar r_{t-1} + \alpha r_t,\\
d_t &= |r_t - \bar r_t|,\\
\bar d_t &= (1-\alpha)\bar d_{t-1} + \alpha d_t,
\end{align}
其中 $\alpha\in(0,1)$ 为 EMA 系数。

\subsection{训练阶段：漂移感知的超参缩放（不改梯度公式）}
当检测到更大的 $\bar d_t$ 时，我们对 SB3 算法的部分超参做温和放缩（\textbf{仅训练阶段}生效）：
\begin{itemize}
  \item PPO：增大 $\texttt{clip\_range}$ 与 $\texttt{target\_kl}$（更允许策略变化，缩短适应延迟）；
  \item A2C：增大 $\texttt{ent\_coef}$（增加探索，避免在旧分布下过早“锁死”动作偏好）。
\end{itemize}
放缩形式可以写为：
\begin{align}
\eta_t = \min(\eta_{\max}, 1 + s \cdot \bar d_t), \qquad \text{hyperparam}_t = \text{hyperparam}_0 \cdot \eta_t,
\end{align}
其中 $s$ 为尺度系数，$\eta_{\max}$ 限制最大放大倍率。

\subsection{实施阶段（红阶段）：严格冻结，只有推理期自适应}
系统硬约束要求红阶段\textbf{禁止任何梯度/优化器更新}。因此我们仅允许\textbf{纯前向}机制改变最终动作。
我们使用一个简单的门控阈值规则（不改变网络参数）：
\begin{align}
g_t &= \mathbb{I}[\bar d_t > \tau_d \;\;\text{or}\;\; \bar r_t < \tau_r],\\
\tau(g_t) &= \begin{cases}
\tau_{\text{low}}, & g_t=1 \\
\tau_{\text{high}}, & g_t=0
\end{cases},\\
a_t &= \mathbb{I}[p_\theta(a=1\mid X_t) \ge \tau(g_t)].
\end{align}
其中 $p_\theta(a=1\mid X_t)$ 来自冻结策略网络的输出概率。该机制属于 inference-time adaptation：\textbf{不更新参数，但允许基于历史统计改变动作判别阈值}。

\textbf{说明：}如果实施阶段系统侧不提供逐步 reward 回传到 RL 线程，则 $\bar r_t,\bar d_t$ 只能在训练/评估阶段更新（门控在红阶段退化为“使用最后一次统计”）。这属于系统接口层面限制，与 HAT 的模型能力无冲突。

\section{工程实现：文件、开关与复现实验}
\subsection{关键代码组件（对应本仓库）}
\begin{itemize}
  \item \textbf{历史 + 注意力模块：}\texttt{codes/robust\_rl/sb3\_attention.py}
    \begin{itemize}
      \item \texttt{HistoryAttentionWrapper}: 将观测改为 $H\times(d+3)$ 序列，并支持 \texttt{keep\_history}。
      \item \texttt{AttentionExtractor}: TransformerEncoder + last-token pooling，输出 $F$ 维特征给 SB3。
    \end{itemize}
  \item \textbf{训练/实施主逻辑与集成：}\texttt{codes/dynamic\_RL34959.py}
    \begin{itemize}
      \item 当环境变量 \texttt{RL\_HAT=1} 且算法为 PPO/A2C 时，自动包裹 env 并注入 \texttt{policy\_kwargs}。
      \item 训练阶段调用 \texttt{model.learn(...)}（SB3 原生），只改变输入特征与少量超参调度（SNA）。
      \item 实施阶段保持 \textbf{no online learning}；动作选择可使用阈值门控版本（SNA 的 inference 部分）。
    \end{itemize}
  \item \textbf{运行入口与算法名映射：}\texttt{codes/Dynamic\_master34959.py}
    \begin{itemize}
      \item 支持 \texttt{--algorithm PPO\_HAT} / \texttt{A2C\_HAT}（以及对应 run 命名），内部设置 \texttt{RL\_HAT=1} 并选择 SB3 基算法。
    \end{itemize}
\end{itemize}

\subsection{关键超参（默认值）}
HAT（模型层）：
\begin{itemize}
  \item 历史长度：$H=20$（\texttt{HATConfig.history\_len}）
  \item Transformer：层数 $L=2$，heads=2，embed\_dim=64，dropout=0.1
  \item 输出特征维：$F=64$（\texttt{HATConfig.feature\_dim}）
  \item keep\_history：默认开启（适配 episode\_length=1）
\end{itemize}
SNA（运行态统计/调度）：
\begin{itemize}
  \item EMA 系数：$\alpha=0.05$（\texttt{HAT\_EMA\_ALPHA}）
  \item 门控阈值：$\tau_d=0.4$，$\tau_r=0.3$（\texttt{HAT\_GATE\_DRIFT\_HI}, \texttt{HAT\_GATE\_REWARD\_LOW}）
  \item 推理期阈值：$\tau_{\text{high}}=0.6$，$\tau_{\text{low}}=0.4$（\texttt{HAT\_TAU\_HIGH}, \texttt{HAT\_TAU\_LOW}）
  \item 调度倍率：$s=1.5$，$\eta_{\max}=3.0$（\texttt{HAT\_DRIFT\_SCALE}, \texttt{HAT\_DRIFT\_MAX\_SCALE}）
\end{itemize}

\subsection{复现实验命令（示例）}
\begin{verbatim}
python codes/Dynamic_master34959.py --dist_name S5_1 --request_number 30 --algorithm A2C_HAT --seed 42 --workers 1
python codes/Dynamic_master34959.py --dist_name S5_1 --request_number 30 --algorithm A2C     --seed 42 --workers 1
\end{verbatim}
每次运行会生成独立 run 目录：\texttt{codes/logs/run\_*/}，包含 \texttt{rl\_trace.csv}、\texttt{rl\_training.csv}、\texttt{rl\_summary.csv} 等。

\section{结果示例（A2C vs A2C\_HAT\_SNA）}
\label{sec:results}
我们以两次 run 的 \texttt{rl\_summary.csv} 为例（测试阶段 200 次决策统计）：
\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
方法 & 平均回报 & 标准差 & 决策数 \\
\midrule
A2C（baseline, S5, R30, seed42） & 0.505 & 0.500 & 200 \\
A2C\_HAT\_SNA（S5, R30, seed42） & 0.805 & 0.396 & 200 \\
\bottomrule
\end{tabular}
\caption{测试阶段平均回报对比（来自对应 run 的 \texttt{rl\_summary.csv}）。}
\end{table}

\textbf{解读：}在该场景中，HAT 通过历史序列与注意力聚合形成上下文特征，使策略能依据近期观测/行动/回报模式做更一致的选择，从而显著提高测试阶段平均回报。

\section{边界与已知限制（如需老师进一步指导的点）}
\begin{itemize}
  \item 若红阶段不提供逐步 reward 回传，任何基于 reward 的门控/漂移检测都无法在线更新，只能依赖观测统计或训练期学习出的内部表征。
  \item 当前 HAT 使用固定窗口 $H$ 与 last-token pooling；后续可以探索更强的池化（attention pooling）或多专家头（MoE）以进一步缩短突变后的翻转延迟（仍保持红阶段无梯度更新）。
\end{itemize}

\end{document}
